---
title: A22/23
slug: DyrnwQs57iVgXkkXv06cIEjKnkf/Jk0Rw4WfBiCHBtkDw0BcSIF0nGh
sidebar_position: 1
---


# A22/23

# Assignment 22

1. Common Aspects:

- Universal Approximators: Both MLP and RBF networks are proven to be universal function approximators, capable of representing any continuous function.
- Feed-Forward Architecture: Both are feed-forward networks where information flows in one direction from input to output.
- Supervised Learning: Both are typically trained using supervised learning methods.
- Application Domains: Both are applicable for function approximation (regression) and pattern recognition (classification) tasks.

1. Different Aspects:

- Layersï¼š
    - MLP: Can have multiple hidden layers.
    - RBF: Has exactly three layers: Input, RBF, and Output.
    - Difference: RBF is a fixed shallow architecture, while MLPs can be deep.

- Hidden Neurons
    - MLP: Uses weighted sums followed by global activation functions
    - RBF: Uses distance calculation and radial basis functions , e.g., Gaussian.
    - Difference: MLP neurons rely on linear combinations, RBF neurons rely on distance measures.

- Output Layer
    - MLP: Typically uses nonâ€‘linear transfer functions , e.g., Sigmoid, Softmax.
    - RBF: Uses a linear transfer function 
    - Difference: RBF output is a simple linear weighted sum, making weight solving mathematically easier.

- Training
    - MLP: Trains all weights simultaneously via Backpropagation .
    - RBF: Often uses hybrid training: centers/sizes fixed or unsupervised; output weights solved by matrix inverse or via gradient descent.
    - Difference: RBF training can be exact and faster because the output layer is a linear problem.

- Approximation
    - MLP: Global approximation, changing one weight affects outputs for many inputs.
    - RBF: Local approximation, changing a center or weight only affects outputs near that center.
    - Difference: RBF allows local tuning without disrupting knowledge in other regions, while MLP adjustments are global.

1. Discussion on Applications:

- RBF Networks are better suited for interpolation problems or applications where training data is sparse but clean. They are also ideal for online learning because their local nature prevents "catastrophic interference" (learning new data doesn't ruin old memories). They excel when training speed is critical due to the possibility of analytic solutions .
- MLPs are better suited for high-dimensional problems. RBFs suffer from the "curse of dimensionality". MLPs are also generally superior for complex classification tasks involving massive datasets where deep feature hierarchies are necessary.

# Assignment 23 

1. Completely Uninformed

- Philosophy: Used when there is little or no prior knowledge about the input data.
- Method: The input space is covered uniformly, e.g., using a uniform grid or tiling. The size $s_k$ is usually set to ensure moderate overlap between centers.
- Pros/Cons: It is easy to implement and guarantees full coverage, but suffers from the curse of dimensionalityâ€”the number of required centers $K$ rises exponentially as input dimensions $N$ increase.

1. Input Data Driven

- Philosophy: The centers are positioned and adjusted based on the distribution of the input data $X$. This is an unsupervised approach.
- Variants:
    - Subset of Training Data: Randomly selecting $K$ points from the training set to serve as centers.
    - Statistical Analysis: Using density analysis of the input space to place centers where data is concentrated.
    - Clustering: Using algorithms like K-means or Vector Quantization. The resulting "codebook vectors"become the RBF centers, providing a good representation of the data distribution.
    - Self-Organized Learning: Using unsupervised learning rules to move centers toward data clusters.

1. Error Driven

- Philosophy: Adjusts parameters based on the final network performance.
- Method: Centers $C_k$ and sizes $s_k$are treated as adjustable weights. The system calculates the output error and uses Gradient Descent to minimize this error.
- Implementation: This requires calculating the partial derivatives of the error function with respect to the centers ($\frac{\partial E}{\partial c_{nk}}$) and the sizes ($\frac{\partial E}{\partial s_{k}}$).

> ### Assignment 22 (3 Points)
> <b>Compare the two neural network paradigms Multi-Layer-Perceptron (MLP) and Radial-Basis-Function Network (RBF).</b>
> 1. <b>Common Aspects:</b>
> - <b>Universal Approximators:</b> Both MLP and RBF networks are proven to be universal function approximators, capable of representing any continuous function(1)(1)(1).
> - <b>Feed-Forward Architecture:</b> Both are feed-forward networks where information flows in one direction from input to output(2).
> - <b>Supervised Learning:</b> Both are typically trained using supervised learning methods (mapping inputs to target outputs)(3).
> - <b>Application Domains:</b> Both are applicable for function approximation (regression) and pattern recognition (classification) tasks(4).
> 1. <b>Different Aspects:</b>
> 2. <b>Discussion on Applications:</b>
> - <b>RBF Networks</b> are better suited for <b>interpolation</b> problems or applications where training data is sparse but clean. They are also ideal for <b>online learning</b> (real-time adaptation) because their local nature prevents "catastrophic interference" (learning new data doesn't ruin old memories). They excel when training speed is critical due to the possibility of analytic solutions (Matrix Inversion).
> - <b>MLPs</b> are better suited for <b>high-dimensional problems</b>. RBFs suffer from the "curse of dimensionality" (the number of centers required grows exponentially with input dimensions)(8). MLPs are also generally superior for complex classification tasks involving massive datasets (like image recognition) where deep feature hierarchies are necessary.
> ---
> # > ### Assignment 23 (2 Points)
> <b>Explain the three philosophies (including variants of them) to set, or adjust the centres </b>$C_k$<b> and sizes </b>$s_k$<b> of RBF-networks.</b>
> The lecture outlines three distinct philosophies based on the amount of prior knowledge available:
> 1. <b>Completely Uninformed</b>
> - <b>Philosophy:</b> Used when there is little or no prior knowledge about the input data(9).
> - <b>Method:</b> The input space is covered <b>uniformly</b> (e.g., using a uniform grid or tiling)(10). The size $s_k$ is usually set to ensure moderate overlap between centers(11).
> - <b>Pros/Cons:</b> It is easy to implement and guarantees full coverage, but suffers from the <b>curse of dimensionality</b>â€”the number of required centers $K$ rises exponentially as input dimensions $N$ increase(12).
> > 1. <b>Input Data Driven</b>
> - <b>Philosophy:</b> The centers are positioned and adjusted based on the <b>distribution</b> of the input data $X$(13). This is an unsupervised approach (does not use teacher/target values).
> - <b>Variants:</b>
    - <b>Subset of Training Data:</b> Randomly selecting $K$ points from the training set to serve as centers(14).
    - <b>Statistical Analysis:</b> Using density analysis of the input space to place centers where data is concentrated(15).
    - <b>Clustering:</b> Using algorithms like <b>K-means</b> or Vector Quantization. The resulting "codebook vectors" (cluster centroids) become the RBF centers, providing a good representation of the data distribution(16).
    -     - <b>Self-Organized Learning:</b> Using unsupervised learning rules to move centers toward data clusters.
> 1. <b>Error Driven</b>
> - <b>Philosophy:</b> Adjusts parameters based on the final network performance (Supervised Learning)(17).
> - <b>Method:</b> Centers $C_k$ and sizes $s_k$ are treated as adjustable weights. The system calculates the output error (difference between network output $y$ and teacher value $\hat{y}$) and uses <b>Gradient Descent</b> to minimize this error.
> - <b>Implementation:</b> This requires calculating the partial derivatives of the error function with respect to the centers ($\frac{\partial E}{\partial c_{nk}}$) and the sizes ($\frac{\partial E}{\partial s_{k}}$)(18).
> è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ã€‚åŒºåˆ†â€œèµ„æ–™æ¥æºâ€å’Œâ€œå¤–éƒ¨çŸ¥è¯†â€å¯¹äºå­¦æœ¯ä¸¥è°¨æ€§å¾ˆé‡è¦ã€‚ä»¥ä¸‹æ˜¯é’ˆå¯¹åˆšæ‰å›ç­”çš„è¯¦ç»†æ¥æºè¯´æ˜ï¼š
> > ### Assignment 22 (å¯¹æ¯” MLP å’Œ RBF)
> è¿™éƒ¨åˆ†å¤§çº¦ <b>60% åŸºäº PPTï¼Œ40% åŸºäºå¤–éƒ¨é€šç”¨çŸ¥è¯†</b>ã€‚
> - <b>âœ… åŸºäº PPT çš„éƒ¨åˆ† (RBF çš„ç‰¹å¾)ï¼š</b>
    - <b>RBF çš„ç»“æ„ï¼š</b> åªæœ‰ 3 å±‚ (Input, RBF, Output) (1)ï¼›è¾“å…¥å±‚æ˜¯çº¿æ€§çš„ (2)ï¼›è¾“å‡ºå±‚ä¹Ÿæ˜¯çº¿æ€§çš„ (Identity) (3)ï¼›RBF ç¥ç»å…ƒä½¿ç”¨è·ç¦»å’Œé«˜æ–¯å‡½æ•° (4)(4)(4)(4)ã€‚
    - <b>åŠŸèƒ½ï¼š</b> é€šç”¨å‡½æ•°é€¼è¿‘å™¨ (5)ï¼Œé€‚ç”¨äºå›å½’å’Œåˆ†ç±» (6)ã€‚
    - <b>è®­ç»ƒï¼š</b> RBF çš„æƒé‡å¯ä»¥é€šè¿‡è§£æè§£ (çŸ©é˜µæ±‚é€†/ä¼ªé€†) ä¸€æ¬¡æ€§è®¡ç®— (7)(7)(7)(7)ï¼Œè¿™æ˜¯ RBF åŒºåˆ«äº MLP çš„ä¸€å¤§ç‰¹ç‚¹ã€‚
    - <b>ç¼ºç‚¹ï¼š</b> æ˜ç¡®æåˆ°äº† RBF çš„â€œç»´æ•°ç¾éš¾â€ (Curse of dimensionality)ï¼Œå³ä¸­å¿ƒç‚¹æ•°é‡éšç»´åº¦æŒ‡æ•°å¢é•¿ (8)ã€‚
> > - <b>ğŸ§  æˆ‘æ·»åŠ çš„å¤–éƒ¨çŸ¥è¯† (MLP çš„ç‰¹å¾ä¸å¯¹æ¯”é€»è¾‘)ï¼š</b>
    - <b>MLP çš„å…·ä½“ç»†èŠ‚ï¼š</b> PPT ä¸­æåˆ°äº† MLP æ˜¯ RBF çš„æ›¿ä»£å“ (9)ï¼Œä½†æ²¡æœ‰è¯¦ç»†å±•å¼€ MLP çš„å†…éƒ¨ç»“æ„ã€‚æˆ‘è¡¥å……äº† MLP ä½¿ç”¨ <b>Sigmoid/ReLU</b> æ¿€æ´»å‡½æ•°ã€é€šå¸¸æœ‰<b>å¤šä¸ªéšè—å±‚</b> (Deep Learning)ã€ä»¥åŠå®ƒæ˜¯åŸºäº<b>ç‚¹ç§¯ (Dot Product)</b> è€Œéè·ç¦»çš„çŸ¥è¯†ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰æ•ˆçš„â€œå¯¹æ¯”â€ã€‚
    - <b>"å…¨å±€ vs å±€éƒ¨" (Global vs Local) é€¼è¿‘ï¼š</b> PPT å±•ç¤ºäº† RBF æ˜¯é«˜æ–¯æ³¢å³° (Local)ï¼Œä½†æˆ‘è¡¥å……äº†â€œMLP æ˜¯å…¨å±€é€¼è¿‘ (Global approximation)â€è¿™ä¸€æœ¯è¯­ï¼Œç”¨æ¥è§£é‡Šä¸ºä»€ä¹ˆ RBF åœ¨è°ƒæ•´å‚æ•°æ—¶åªä¼šå½±å“å±€éƒ¨åŒºåŸŸï¼Œè€Œ MLP ä¼šå½±å“æ•´ä½“ã€‚
    - <b>åº”ç”¨åœºæ™¯çš„æ¨è®ºï¼š</b> PPT æåˆ°äº† RBF ä¸é€‚åˆé«˜ç»´ (ç»´æ•°ç¾éš¾)ï¼Œæˆ‘æ®æ­¤æ¨è®ºå‡º MLP æ›´é€‚åˆé«˜ç»´ä»»åŠ¡ï¼ˆå¦‚å›¾åƒè¯†åˆ«ï¼‰ã€‚æˆ‘ä¹Ÿè¡¥å……äº† RBF é€‚åˆâ€œåœ¨çº¿å­¦ä¹  (Online Learning)â€çš„è§‚ç‚¹ï¼Œè¿™æ˜¯åŸºäºå…¶å±€éƒ¨ç‰¹æ€§æ¨å¯¼å‡ºçš„å¸¸è§å­¦æœ¯è§‚ç‚¹ã€‚
> ---
> # > ### Assignment 23 (è®¾ç½® RBF ä¸­å¿ƒçš„æ–¹æ³•)
> è¿™éƒ¨åˆ† <b>99% åŸºäº PPT</b>ã€‚
> - <b>âœ… åŸºäº PPT çš„éƒ¨åˆ†ï¼š</b>
    - <b>ä¸‰ç§å“²å­¦ï¼š</b> Uninformed, Input Data Driven, Error Driven è¿™ä¸‰ä¸ªåˆ†ç±»å®Œå…¨æ¥è‡ª PPT (10)ã€‚
    - <b>Uninformed (å®Œå…¨æ— çŸ¥)ï¼š</b> â€œå‡åŒ€è¦†ç›– (Uniformly covered)â€ (11)å’Œå…¶ç¼ºç‚¹ (ç»´æ•°ç¾éš¾) (12) éƒ½æ¥è‡ª PPTã€‚
    - <b>Input Data Driven (æ•°æ®é©±åŠ¨)ï¼š</b> å››ç§å˜ä½“ï¼ˆå­é›† Subsetã€ç»Ÿè®¡åˆ†æ Statisticalã€èšç±» Clusteringã€å­¦ä¹  Learningï¼‰éƒ½ç›´æ¥åˆ—åœ¨ PPT ä¸Š (13)ã€‚"Codebook-vectors" è¿™ä¸ªæœ¯è¯­ä¹Ÿæ˜¯ PPT é‡Œçš„åŸè¯ (14)ã€‚
    - <b>Error Driven (è¯¯å·®é©±åŠ¨)ï¼š</b> ä½¿ç”¨ç›‘ç£å­¦ä¹ å’Œæ¢¯åº¦ä¸‹é™ (15)ï¼Œä»¥åŠéœ€è¦è®¡ç®—åå¯¼æ•° $\frac{\partial E}{\partial c}$ å’Œ $\frac{\partial E}{\partial s}$ çš„å…¬å¼å½¢å¼ï¼Œå‡æ¥è‡ª PPT (16)ã€‚
    > - <b>ğŸ§  æˆ‘æ·»åŠ çš„å¤–éƒ¨çŸ¥è¯†ï¼š</b>
    - ä»…åœ¨è§£é‡Š <b>Clustering (èšç±»)</b> æ—¶ï¼Œæˆ‘ç¨å¾®å±•å¼€äº†è§£é‡Šäº†ä¸€ä¸‹ K-means æ˜¯å¦‚ä½•æ‰¾åˆ°â€œè´¨å¿ƒâ€çš„ï¼Œä»¥å¸®åŠ©ç†è§£ PPT ä¸­æåˆ°çš„ "Codebook vectors" æ˜¯ä»€ä¹ˆæ„æ€ã€‚
> <b>æ€»ç»“ï¼š</b>
> - <b>Assignment 23</b> ä½ å®Œå…¨å¯ä»¥ä¿¡èµ– PPT çš„å†…å®¹æ¥å›ç­”ã€‚
> - <b>Assignment 22</b> ä½ éœ€è¦åˆ©ç”¨ PPT ä¸­å…³äº RBF çš„æè¿°ï¼Œä½†å¿…é¡»ç»“åˆä½ ä¹‹å‰å­¦è¿‡çš„å…³äº MLP çš„çŸ¥è¯†ï¼ˆæˆ–è€…æˆ‘è¡¥å……çš„è¿™äº›çŸ¥è¯†ï¼‰æ‰èƒ½å®Œæˆâ€œå¯¹æ¯” (Compare)â€çš„éƒ¨åˆ†ï¼Œå› ä¸ºè¿™ä»½ PPT ä¸»è¦è®² RBFï¼Œå‡ ä¹æ²¡è®² MLP çš„ç»†èŠ‚ã€‚

