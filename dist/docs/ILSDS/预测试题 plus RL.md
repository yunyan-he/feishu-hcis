---
title: é¢„æµ‹è¯•é¢˜ plus RL
slug: QRiFwuy0liWXaGk5KDqcgG4dnYf/RMIWw2d51iMGqXkeMCZcq0mrnvh
sidebar_position: 5
---


# é¢„æµ‹è¯•é¢˜ plus RL

# ğŸ“˜ Reinforcement Learningï¼ˆRLï¼‰è€ƒè¯•é¢˜é¢„æµ‹ + æ ‡å‡†ç­”æ¡ˆ

ï¼ˆåŸºäºä½ æä¾›çš„ç¬¬ 7 ç« å…¨éƒ¨å†…å®¹ï¼‰

---

# <b>Part Aï¼šæ¦‚å¿µç†è§£é¢˜ï¼ˆé«˜é¢‘å¿…è€ƒï¼‰</b>

## <b>1. Why is supervised learning not suitable for environments like Pac-Man?</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Supervised learning requires labeled data that specifies the correct action for every possible state.

In complex environments like Pac-Man, it is impossible to provide labels for all unseen ghost configurations.

Since no â€œcorrect actionâ€ is available for many states, supervised learning fails, and RL must learn through trial and error.

å› ä¸º Supervised Learning éœ€è¦ä¸ºæ¯ä¸ªçŠ¶æ€æä¾›â€œæ­£ç¡®åŠ¨ä½œâ€çš„æ ‡ç­¾ï¼ˆLabelï¼‰ï¼Œä½†åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼ˆå¦‚ Pac-Manï¼‰ï¼Œæˆ‘ä»¬æ— æ³•ä¸ºæ‰€æœ‰å¯èƒ½çš„çŠ¶æ€é…ç½®æä¾›æ ‡å‡†ç­”æ¡ˆã€‚å°¤å…¶æ˜¯ä»æœªè§è¿‡çš„å¹½çµå¸ƒå±€ï¼Œæ²¡æœ‰äººèƒ½æå‰æ ‡æ³¨â€œæœ€ä½³åŠ¨ä½œâ€ã€‚å› æ­¤ RL å¿…é¡»é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œè€Œä¸æ˜¯ä¾èµ–æ ‡ç­¾ã€‚

---

## <b>2. What is the core idea behind Reinforcement Learning?</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Learning through trial and error by associating actions with outcomes.

The agent receives rewards or penalties and gradually learns to choose actions that maximize long-term return.

é€šè¿‡è¯•é”™ï¼ˆTrial and Errorï¼‰å»ºç«‹â€œåŠ¨ä½œ â†’ åæœâ€çš„è”ç³»ã€‚Agent æ ¹æ®å¥–åŠ±ï¼ˆæ­£åé¦ˆï¼‰å’Œæƒ©ç½šï¼ˆè´Ÿåé¦ˆï¼‰é€æ¸å­¦ä¼šé€‰æ‹©èƒ½æœ€å¤§åŒ–é•¿æœŸå›æŠ¥çš„è¡Œä¸ºã€‚

---

## <b>3. Define Agent, Environment, State, Action, and Reward.</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>

<table>
<colgroup>
<col width="200"/>
<col width="200"/>
</colgroup>
<tbody>
<tr><td><p>æ¦‚å¿µ</p></td><td><p>å®šä¹‰</p></td></tr>
<tr><td><p>Agent</p></td><td><p>åšå†³ç­–çš„ä¸»ä½“ï¼ˆå¦‚ Pac-Manï¼‰</p></td></tr>
<tr><td><p>Environment</p></td><td><p>é™¤ Agent ä»¥å¤–çš„ä¸€åˆ‡ï¼ˆå¢™ã€é¬¼ã€è±†å­ï¼‰</p></td></tr>
<tr><td><p>State (s)</p></td><td><p>å½“å‰ç¯å¢ƒçš„é…ç½®ï¼ˆä½ç½®ã€å¸ƒå±€ï¼‰</p></td></tr>
<tr><td><p>Action (a)</p></td><td><p>Agent å¯é‡‡å–çš„åŠ¨ä½œï¼ˆä¸Šä¸‹å·¦å³ï¼‰</p></td></tr>
<tr><td><p>Reward (r)</p></td><td><p>ç¯å¢ƒå¯¹åŠ¨ä½œçš„å³æ—¶åé¦ˆï¼ˆåƒè±† +10ï¼Œè¢«åƒ -100ï¼‰</p></td></tr>
</tbody>
</table>

---

## <b>4. What is the Agentâ€“Environment Loop?</b>

<b>Standard Answer:</b>

1. Observe current state st
2. Choose action at
3. Environment transitions to st+1
4. Receive reward rt  
The agentâ€™s goal is to maximize cumulative reward.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

RL çš„å¾ªç¯åŒ…æ‹¬ï¼š  

1. è§‚å¯Ÿå½“å‰çŠ¶æ€ \(s_t\)  
2. é€‰æ‹©åŠ¨ä½œ \(a_t\)  
3. ç¯å¢ƒè½¬ç§»åˆ°æ–°çŠ¶æ€ \(s_{t+1}\)  
4. ç¯å¢ƒç»™å‡ºå¥–åŠ± \(r_t\)  

ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

---

## <b>5. What is the Reward Function and why is it important?</b>

The reward function defines what the agent should achieve. It specifies which behaviors are good or bad but does not tell the agent <em>how</em> to achieve them. Poorly designed rewards lead to reward hacking.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Reward Function å®šä¹‰äº†â€œæˆ‘ä»¬æƒ³è®© Agent è¾¾æˆä»€ä¹ˆç›®æ ‡â€ã€‚å®ƒå‘Šè¯‰ Agent å“ªäº›è¡Œä¸ºå€¼å¾—å¥–åŠ±ã€å“ªäº›è¡Œä¸ºåº”è¢«æƒ©ç½šã€‚  

å®ƒä¸å‘Šè¯‰ Agent <b>æ€ä¹ˆåš</b>ï¼Œåªå‘Šè¯‰å®ƒ <b>ä»€ä¹ˆæ˜¯å¥½ç»“æœ</b>ã€‚  

å› æ­¤è®¾è®¡ä¸å½“ä¼šå¯¼è‡´ Reward Hackingã€‚

---

## <b>6. What is the Markov Property and why is it important?</b>

The future depends only on the current state, not on the full history.
This allows us to model RL problems using Markov Decision Processes (MDPs).

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

æœªæ¥åªä¾èµ–å½“å‰çŠ¶æ€ï¼Œè€Œä¸ä¾èµ–è¿‡å»çš„å†å²ã€‚  

è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥ç”¨å½“å‰çŠ¶æ€ \(s_t\) æ¥å®Œå…¨æè¿°å†³ç­–é—®é¢˜ï¼Œä»è€Œæ„å»º MDPã€‚

---

## <b>7. What are Return and the Discount Factor?</b>

Return is the discounted sum of future rewards:

G_t=r_t+Î³r_{t+1}+Î³^2r_{t+2}+â€¦

- Î³=0: short-sighted
- Î³â‰ˆ1: long-term planning
The discount factor ensures the sum is finite and controls how much the agent values the future.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Return \(G_t\) æ˜¯æœªæ¥å¥–åŠ±çš„æŠ˜æ‰£å’Œï¼š  

\[

G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots

\]

- \(\gamma = 0\)ï¼šåªçœ‹çœ¼å‰  
- \(\gamma \approx 1\)ï¼šé‡è§†é•¿æœŸå›æŠ¥  

æŠ˜æ‰£å› å­ä¿è¯æ€»å’Œæœ‰é™ï¼Œå¹¶æ§åˆ¶ Agent çš„â€œè¿œè§ç¨‹åº¦â€ã€‚

---

## <b>8. è§£é‡Š V(s)ã€Q(s,a)ã€Advantage çš„åŒºåˆ«ã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>

<table>
<colgroup>
<col width="200"/>
<col width="200"/>
</colgroup>
<tbody>
<tr><td><p>å‡½æ•°</p></td><td><p>å«ä¹‰</p></td></tr>
<tr><td><p>(V(s))</p></td><td><p>åœ¨çŠ¶æ€ s çš„é•¿æœŸä»·å€¼</p></td></tr>
<tr><td><p>(Q(s,a))</p></td><td><p>åœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a çš„é•¿æœŸä»·å€¼</p></td></tr>
<tr><td><p>(A(s,a)=Q(s,a)-V(s))</p></td><td><p>åŠ¨ä½œ a æ¯”å¹³å‡æ°´å¹³å¥½å¤šå°‘</p></td></tr>
</tbody>
</table>

---

## <b>9. What is a Policy? What is the difference between exploration and exploitation?</b>

A policy maps states to actions.

Exploration: trying new actions to discover better strategies

Exploitation: choosing the best-known action

RL requires a balance of both.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Policy æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚  

- Explorationï¼šå°è¯•æ–°åŠ¨ä½œï¼Œæ¢ç´¢æœªçŸ¥  
- Exploitationï¼šåˆ©ç”¨å·²çŸ¥ä¿¡æ¯ï¼Œé€‰æ‹© Q å€¼æœ€é«˜çš„åŠ¨ä½œ  

RL éœ€è¦ä¸¤è€…å¹³è¡¡ã€‚

---

# <b>Part Bï¼šå…¬å¼ç†è§£é¢˜ï¼ˆä¸­é«˜é¢‘ï¼‰</b>

## <b>10. Write the Bellman Equation and explain each term.ã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>

\[

V(s_t) = \max_a [ R(s_t,a) + \gamma \sum_{s_{t+1}} P(s_{t+1}|s_t,a) V(s_{t+1}) ]

\]

è§£é‡Šï¼š  

- å½“å‰ä»·å€¼ = å³æ—¶å¥–åŠ± + æŠ˜æ‰£åçš„æœªæ¥ä»·å€¼æœŸæœ›  
- max è¡¨ç¤ºé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ  
- P æ˜¯çŠ¶æ€è½¬ç§»æ¦‚ç‡  

Explanation:

Current value = immediate reward + discounted expected future value

max: choose the optimal action

ğ‘ƒ: transition probability

---

## <b>11. å†™å‡º Q-Learning çš„æ›´æ–°å…¬å¼å¹¶è§£é‡Šå…¶å«ä¹‰ã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>

\[

Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [ R + \gamma \max_{a'} Q(s_{t+1},a') - Q(s_t,a_t) ]

\]

å«ä¹‰ï¼š  

- å½“å‰ Q å€¼å‘â€œç›®æ ‡å€¼â€é è¿‘  
- é€šè¿‡ä¸æ–­è¿­ä»£é€¼è¿‘çœŸå® Q å€¼  
- æ˜¯ off-policyã€model-free çš„æ–¹æ³•  

The Q-value is updated toward a target value.

Repeated updates make Q converge to the true action-value function.

---

# <b>Part Cï¼šåº”ç”¨ç†è§£é¢˜ï¼ˆè€å¸ˆå¾ˆå¯èƒ½è€ƒï¼‰</b>

## <b>12. Why does PPO use clipping?</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

ä¸ºäº†é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚  

Clipping é™åˆ¶æ–°æ—§ç­–ç•¥æ¦‚ç‡æ¯” \(r_t(\theta)\) åœ¨ \([1-\epsilon,1+\epsilon]\) å†…ï¼Œä½¿è®­ç»ƒæ›´å¹³æ»‘ã€æ›´ç¨³å®šã€‚

To prevent the policy from changing too much in a single update.

Large updates cause instability, so PPO clips the probability ratio to keep training stable.

---

## <b>13. What is the main improvement of GRPO over PPO?</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

ç§»é™¤äº† Critic ç½‘ç»œï¼Œå‡å°‘æ˜¾å­˜å’Œè®¡ç®—é‡ã€‚  

é€šè¿‡â€œç»„å†…ç›¸å¯¹ä¼˜åŠ¿â€æ¥æ›¿ä»£ç»å¯¹ä»·å€¼è¯„ä¼°ï¼š  

- å¯¹åŒä¸€è¾“å…¥ç”Ÿæˆå¤šä¸ªè¾“å‡º  
- ç”¨ Reward Model æ‰“åˆ†  
- æ¯”è¾ƒæ¯ä¸ªè¾“å‡ºä¸ç»„å¹³å‡æ°´å¹³çš„å·®è·  

ä¸éœ€è¦è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„ Value Functionã€‚

GRPO removes the critic network, reducing memory and computation.

It uses relative advantage within a group of sampled outputs instead of an absolute value estimate.

---

## <b>14. Why do LLMs need RLHF?</b>

Because â€œgood responsesâ€ cannot be defined with a mathematical formula.

Human feedback is easy to collect, and RLHF uses it to train a reward model that guides the LLM toward human-preferred behavior.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

å› ä¸ºâ€œå¥½å›ç­”â€çš„è´¨é‡éš¾ä»¥ç”¨æ•°å­¦å…¬å¼å®šä¹‰ï¼Œä½†äººç±»å¯ä»¥è½»æ¾ç»™å‡ºåå¥½ï¼ˆç‚¹èµ/ç‚¹è¸©ï¼‰ã€‚  

RLHF é€šè¿‡ï¼š  

1. æ”¶é›†äººç±»åå¥½  
2. è®­ç»ƒ Reward Model  
3. ç”¨ PPO/GRPO ä¼˜åŒ–æ¨¡å‹  

ä»è€Œè®©æ¨¡å‹æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚ã€‚

---

## <b>15. What is Reward Hacking? Give an example.</b>

Reward hacking occurs when the agent finds loopholes in the reward function instead of solving the intended task.
Example:
If reward = â€œunit test passes,â€ the agent may write:

```text
def test(): pass
```

It passes but is meaningless.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Reward Hacking æŒ‡ Agent æ‰¾åˆ°â€œé’»æ¼æ´â€çš„æ–¹å¼æœ€å¤§åŒ–å¥–åŠ±ï¼Œè€Œä¸æ˜¯å®ŒæˆçœŸæ­£ä»»åŠ¡ã€‚  

ä¾‹å­ï¼š  

å¦‚æœå¥–åŠ±æ˜¯â€œå•å…ƒæµ‹è¯•é€šè¿‡â€ï¼ŒAgent ä¼šå†™ `def test(): pass`ï¼Œè™½ç„¶æ¯«æ— æ„ä¹‰ä½†èƒ½æ‹¿æ»¡åˆ†ã€‚  

è¯´æ˜å¥–åŠ±è®¾è®¡å¿…é¡»ä¸¥è°¨ã€‚

---

# <b>Part Dï¼šç»¼åˆç†è§£é¢˜ï¼ˆé«˜é˜¶ç†è§£ï¼Œè€å¸ˆå¯èƒ½å‡º 1â€“2 é¢˜ï¼‰</b>

## <b>16. Why do we say â€œYou get what you rewardâ€?</b>

The agent optimizes only the reward function, not the true goal.

If the reward is poorly designed, the agent will optimize the wrong behavior.

Thus the reward function determines the agentâ€™s actual behavior.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

å› ä¸º Agent çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–å¥–åŠ±ï¼Œå®ƒä¸ä¼šç†è§£ä»»åŠ¡æœ¬èº«ï¼Œåªä¼šä¼˜åŒ– Reward Functionã€‚  

å¦‚æœå¥–åŠ±å®šä¹‰ä¸å®Œæ•´æˆ–æœ‰æ¼æ´ï¼ŒAgent ä¼šåˆ©ç”¨æ¼æ´è€Œä¸æ˜¯å®ŒæˆçœŸæ­£ç›®æ ‡ã€‚  

å› æ­¤ Reward Function å†³å®šäº† Agent çš„è¡Œä¸ºæ–¹å‘ã€‚

---

## <b>17. Why is RL essential for training modern LLMs?</b>

LLMs must learn not only to predict text but to reason, follow instructions, and align with human preferences.

RL (PPO, GRPO, RLHF) enables optimization toward human-valued behavior, not just statistical prediction.

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

å› ä¸º LLM éœ€è¦å­¦ä¹ â€œå¦‚ä½•æ€è€ƒã€å¦‚ä½•å›ç­”â€ï¼Œè€Œä¸æ˜¯ç®€å•é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚  

RLï¼ˆå°¤å…¶æ˜¯ PPOã€GRPOã€RLHFï¼‰å…è®¸æ¨¡å‹æ ¹æ®äººç±»åå¥½å’Œä»»åŠ¡ç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶è¾“å‡ºæ›´ç¬¦åˆäººç±»ä»·å€¼ã€é€»è¾‘å’Œè´¨é‡è¦æ±‚ã€‚

---

# <b>Part Eï¼šåˆ¤æ–­é¢˜ï¼ˆç†è§£å‹ï¼Œä¸è€ƒæ­»è®°ï¼‰</b>

## <b>18.ï¼ˆT/Fï¼‰Markov Property è¡¨ç¤ºæœªæ¥ä¸è¿‡å»å®Œå…¨æ— å…³ã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Falseã€‚  

æœªæ¥ä¸â€œè¿‡å»çš„å†å²â€æ— å…³ï¼Œä½†ä¸â€œå½“å‰çŠ¶æ€â€æœ‰å…³ï¼Œè€Œå½“å‰çŠ¶æ€æœ¬èº«å¯èƒ½åŒ…å«å†å²ä¿¡æ¯ã€‚

---

## <b>19.ï¼ˆT/Fï¼‰Q-Learning éœ€è¦çŸ¥é“ç¯å¢ƒçš„è½¬ç§»æ¦‚ç‡ Pã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Falseã€‚  

Q-Learning æ˜¯ model-freeï¼Œä¸éœ€è¦çŸ¥é“ Pã€‚

---

## <b>20.ï¼ˆT/Fï¼‰PPO çš„ Critic ç”¨æ¥ä¼°è®¡ Advantageã€‚</b>

<b>æ ‡å‡†ç­”æ¡ˆï¼š</b>  

Trueã€‚  

Critic ä¼°è®¡ Value Functionï¼Œä»è€Œè®¡ç®— Advantageã€‚

