---
title: é¢„æµ‹è¯•å· L4
slug: QQqNwziPmiYyFBknLftcE8vWnLb/WJZ2wjquYitDpckdlLZcuKbcnsb
sidebar_position: 2
---


# é¢„æµ‹è¯•å· L4

å½“ç„¶å¯ä»¥ï¼Œäº‘é›ã€‚æˆ‘ç»™ä½ å‡†å¤‡ä¸€ä»½<b>å®Œå…¨é’ˆå¯¹ Lecture 4ï¼ˆLatent Pattern Mining Iï¼‰</b> çš„ <b>è€ƒè¯•é¢˜é¢„æµ‹ + æ ‡å‡†ç­”æ¡ˆ</b>ã€‚  

é¢˜ç›®é£æ ¼ä¸¥æ ¼è´´åˆ Sparrenberg è€å¸ˆçš„å‡ºé¢˜æ–¹å¼ï¼š  

- <b>é‡ç‚¹æ¦‚å¿µé¢˜ï¼ˆé«˜æ¦‚ç‡ï¼‰</b>  
- <b>ç†è§£æ€§é¢˜ï¼ˆä¸­é«˜æ¦‚ç‡ï¼‰</b>  
- <b>è®¡ç®—é¢˜ï¼ˆçŸ©é˜µåˆ†è§£ / CF / SVDï¼‰ï¼ˆé«˜æ¦‚ç‡ï¼‰</b>  
- <b>ä¸ä¼šè€ƒæ­»è®°ç¡¬èƒŒçš„ç»†èŠ‚</b>  
- <b>æ¯é¢˜éƒ½ç»™å‡ºæ ‡å‡†è‹±æ–‡ç­”æ¡ˆï¼ˆè€ƒè¯•å¯ç›´æ¥å†™ï¼‰</b>
    
ä½ å¯ä»¥ç›´æ¥æ‹¿å»å¤ä¹ ã€‚

---

# ğŸ“˜ <b>Lecture 4 â€” Exam Prediction + Model Answers</b>

---

# ğŸ… Part A â€” Core Concept Questionsï¼ˆé«˜æ¦‚ç‡ï¼‰

---

## <b>Q1. Define High Utility Itemset Mining (HUI). Why is it needed?</b>

### <b>Model Answer</b>

High Utility Itemset Mining aims to find itemsets whose total utility in the database exceeds a minimum utility threshold.  

Unlike frequent itemset mining, which only considers occurrence frequency, HUI incorporates both local utility (quantity in a transaction) and global utility (item value).  

It is needed because high-value item combinations may appear infrequently but still be important for applications such as marketing and revenue analysis.

---

## <b>Q2. Explain why the Apriori principle does not apply to High Utility Itemset Mining.</b>

### <b>Model Answer</b>

The Apriori principle relies on downward closure: all subsets of a frequent itemset must also be frequent.  

However, in HUI, utility is not monotonic. A superset may have higher utility than its subsets, and a subset may have low utility even if the superset is high-utility.  

Therefore, Apriori pruning cannot be directly applied.

---

## <b>Q3. What is Transaction-Weighted Utility (TWU) and why is it useful?</b>

### <b>Model Answer</b>

TWU of an itemset is the sum of the transaction weights of all transactions containing the itemset.  

Transaction weight is the total utility of all items in a transaction.  

TWU is useful because it satisfies a downward closure property: the TWU of a subset is always greater than or equal to the TWU of its supersets.  

This allows pruning of itemsets that cannot possibly reach the minimum utility threshold.

---

## <b>Q4. Define collaborative filtering and list its main categories.</b>

### <b>Model Answer</b>

Collaborative filtering is a recommendation approach that predicts user preferences by leveraging similarities among users or items.  

Its main categories are:

- Neighborhood-oriented CF  
- Model-oriented CF  
- Composite CF  
    
---

## <b>Q5. What is matrix factorization in the context of recommender systems?</b>

### <b>Model Answer</b>

Matrix factorization decomposes the userâ€“item matrix into two low-rank matrices:  

$$

X \approx C P^T

$$  

where \(C\) contains item factors and \(P\) contains user factors.  

It discovers latent patterns and enables dimensionality reduction, improving recommendation accuracy.

---

# ğŸ…‘ Part B â€” Understanding & Interpretation Questionsï¼ˆä¸­é«˜æ¦‚ç‡ï¼‰

---

## <b>Q6. Explain the idea of neighborhood-oriented collaborative filtering.</b>

### <b>Model Answer</b>

Neighborhood-oriented CF predicts a userâ€™s preference for an item by aggregating the preferences of similar users (user-based) or similar items (item-based).  

Similarity is often computed using cosine similarity on co-rated items.  

The final prediction is a weighted average of neighbor ratings.

---

## <b>Q7. Describe the Alternating Least Squares (ALS) method for matrix factorization.</b>

### <b>Model Answer</b>

ALS alternates between optimizing one factor matrix while keeping the other fixed.  

Steps:  

1. Initialize \(C\) and \(P\).  
2. Fix \(P\), solve for \(C\) using least squares.  
3. Fix \(C\), solve for \(P\).  
4. Repeat until convergence.  

This avoids solving a non-convex joint optimization directly.

---

## <b>Q8. What is the purpose of RRSS in SVD?</b>

### <b>Model Answer</b>

RRSS (Relative Residual Sum of Squares) measures how much information is lost when approximating a matrix using only the top \(k\) singular values.  

A lower RRSS indicates a better low-rank approximation.  

It helps determine an appropriate rank \(k\).

---

# ğŸ…’ Part C â€” Calculation Questionsï¼ˆé«˜æ¦‚ç‡ï¼‰

---

## <b>Q9. Compute the utility of an itemset.</b>

Given:  

- Global utilities: A=2, B=5  
- Transaction T contains: (A, 3), (B, 1)
    
### <b>Model Answer</b>

$$

u(A,T) = 3 \cdot 2 = 6

$$

$$

u(B,T) = 1 \cdot 5 = 5

$$

$$

u(\{A,B\},T) = 6 + 5 = 11

$$

---

## <b>Q10. Compute TWU of an itemset.</b>

Given:  

- Transaction weights:  
    - T1 = 10  
    - T3 = 7  

- Itemset {A,B} appears in T1 and T3
    
### <b>Model Answer</b>

$$

TWU(\{A,B\}) = 10 + 7 = 17

$$

---

## <b>Q11. Neighborhood CF prediction</b>

Given:  

- Neighbors of user u: users b1, b2  
- Similarities: sim(u,b1)=0.8, sim(u,b2)=0.6  
- Ratings for item j: f_{jb1}=4, f_{jb2}=2  
    
Compute predicted rating.

### <b>Model Answer</b>

$$

\hat{f}_{ju} = \frac{0.8 \cdot 4 + 0.6 \cdot 2}{0.8 + 0.6}

= \frac{3.2 + 1.2}{1.4}

= \frac{4.4}{1.4}

= 3.14

$$

---

## <b>Q12. Matrix factorization reconstruction</b>

Given:  

- Item factor \(C_j = [0.9, 0.3]\)  
- User factor \(P_u = [0.7, 0.5]\)
    
Compute predicted rating.

### <b>Model Answer</b>

$$

\hat{f}_{ju} = C_j \cdot P_u = 0.9 \cdot 0.7 + 0.3 \cdot 0.5 = 0.63 + 0.15 = 0.78

$$

---

# ğŸ…“ Part D â€” SVD Questionsï¼ˆä¸­é«˜æ¦‚ç‡ï¼‰

---

## <b>Q13. What is truncated SVD and why is it used?</b>

### <b>Model Answer</b>

Truncated SVD keeps only the top \(k\) singular values and corresponding singular vectors.  

It provides a low-rank approximation of the matrix, capturing the most important latent structure while reducing noise and dimensionality.  

It is useful for recommender systems and latent pattern mining.

---

## <b>Q14. Explain how SVD relates to matrix factorization.</b>

### <b>Model Answer</b>

SVD decomposes a matrix as \(X = U S V^T\).  

By taking the top \(k\) singular values and defining  

$$

C = U S^{1/2},\quad P = V S^{1/2},

$$  

we obtain a factorization \(X \approx C P^T\).  

Thus, SVD provides a principled way to obtain latent factors.

---

# ğŸ…” Part E â€” Higher-Level Reasoningï¼ˆè€å¸ˆå–œæ¬¢çš„ç†è§£é¢˜ï¼‰

---

## <b>Q15. Why is latent pattern mining important for recommender systems?</b>

### <b>Model Answer</b>

Userâ€“item data is typically sparse, noisy, and high-dimensional.  

Latent pattern mining uncovers hidden structuresâ€”such as user preferences or item categoriesâ€”that are not directly observable.  

These latent factors improve prediction accuracy, reduce dimensionality, and enable generalization to unseen data.

---

# ğŸ¯ æœ€ç»ˆæ€»ç»“ï¼šLecture 4 æœ€å¯èƒ½è€ƒä»€ä¹ˆï¼Ÿ

### <b>å¿…è€ƒï¼ˆ100%ï¼‰</b>

- HUIï¼šlocal utility / global utility / transactional utility  
- ä¸ºä»€ä¹ˆ Apriori ä¸é€‚ç”¨äº HUI  
- TWU + downward closure  
- Collaborative Filteringï¼ˆneighborhood vs model-basedï¼‰  
- Matrix Factorizationï¼ˆX â‰ˆ CPáµ€ï¼‰  
- ALS åŸç†  
- SVD + truncated SVD + RRSS  
    
### <b>é«˜æ¦‚ç‡è®¡ç®—é¢˜</b>

- utility  
- TWU  
- CF weighted average  
- MF dot product  
- SVD ä½ç§©è¿‘ä¼¼æ¦‚å¿µé¢˜  
    
### <b>ä¸­ç­‰æ¦‚ç‡</b>

- æ¨èç³»ç»Ÿç±»å‹  
- offline vs online evaluation  
    
---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘è¿˜èƒ½å¸®ä½ ï¼š

- åšä¸€ä»½ <b>Lecture 4 çš„ä¸€é¡µçº¸é€Ÿè®°ç‰ˆï¼ˆcheat sheetï¼‰</b>  
- åšä¸€ä»½ <b>Lecture 2â€“4 ç»¼åˆå¤§é¢˜é¢„æµ‹</b>  
- åšä¸€ä»½ <b>çº¯è®¡ç®—é¢˜è®­ç»ƒå·ï¼ˆå«ç­”æ¡ˆï¼‰</b>
    
ä½ æƒ³ç»§ç»­å“ªä¸€ä¸ªï¼Ÿ

