---
title: TNN
slug: DyrnwQs57iVgXkkXv06cIEjKnkf
sidebar_position: 1
---


# TNN

## Motivation, Why? 

### Interdisciplinary Interest in Neural Networks

ä¸åŒå­¦ç§‘å‡ºäºä¸åŒç›®çš„ç ”ç©¶ç¥ç»ç½‘ç»œï¼š

---

### Neural Networks as a â€œHot Topicâ€

- åœ¨è¿‡å»çš„ 5â€“7 å¹´ä¸­ï¼Œç¥ç»ç½‘ç»œæŠ€æœ¯ï¼ˆå°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼‰åœ¨ç§‘å­¦ç•Œå’Œå…¬ä¼—ä¸­å¹¿æ³›ä¼ æ’­ã€‚Most of these applications are based on the neural network  paradigmsï¼š Deep learning and Convolutional Neural Networks
- è¿™äº›æŠ€æœ¯æ¨åŠ¨äº†å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„çªç ´ã€‚
- å› æ­¤ï¼Œç¥ç»ç½‘ç»œæˆä¸ºå½“å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ç ”ç©¶çƒ­ç‚¹ã€‚

---

### Biological Systems as Inspiration

ç”Ÿç‰©ä¿¡æ¯å¤„ç†ç³»ç»Ÿå±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼š

-  execute tasks in real timeï¼š
    - Pattern Recognition
    - Speech Recognition & Production
    - Face Recognition
    - é¿éšœä¸è½¨è¿¹è§„åˆ’ï¼ˆObstacle Avoidance & Trajectory Generationï¼‰
    - æ§åˆ¶ä»»åŠ¡ï¼ˆControl Tasksï¼‰

-  show the propertyï¼š
    - Learning
    - Generalization
    - è‡ªç»„ç»‡ï¼ˆSelf-organizationï¼‰
    - æŸä¼¤å®¹å¿ï¼ˆLesion Toleranceï¼‰

---

### Limitations of von Neumann Architecture

- Today's computer architectures are still mostly based on the  von-Neumann-principle, good on computing tasks
- ä½†åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®ã€æ„ŸçŸ¥ä»»åŠ¡ç­‰æ–¹é¢è¡¨ç°ä¸ä½³ã€‚
- theory claims, that the essential difference  between nowadays computers and biological neural  networks are the underlying principles that govern the  structure

---

### Desired Properties from Biology

we would like to have as well for our technical  devices:ï¼š

## Biological background  

### Discovery of the Brain as a Neural Network

since the last part of the 19th century human found that the brain is a highly structured network of single cells (neurons) that  are communicating among each other. 

Discover:

1. The first functional classes of neurons
2. single  functional regions of the brain 
3. The Brain as the Central Processing Unit
    - The most prominent part of the central nervous system is the brain.
    - The central nervous system is a highly structured network of  interconnected processing units (neurons)
    - The structure of a single neuron is simple but each neuron of it's own is a  complicated and powerful computing unit
    - A simplified neuron is  the basis for the development of the technical neurons used in most technical neural networks

4. The shape aligned with the functionality of  neurons
    - Pyramid cellsï¼ˆé‡‘å­—å¡”ç»†èƒï¼‰
    - Purkinje cellsï¼ˆæµ¦è‚¯é‡ç»†èƒï¼‰ï¼šæ ‘çŠ¶ç»“æ„å¤æ‚ï¼Œä½äºå°è„‘

### Important rule borrowed from biological role model

1. A lot of simple, interconnected processing units(neurons)
2. Modifiable, plastic connections(synapses, synaptic weighs)
3. Mapping from an input vector to a scalar output value
4. Nonlinear transfer function

## History of technical neural network research  

äº”ä¸ªå†å²é˜¶æ®µï¼š

1. The threshold-ed binary neural switching element proposed by McCulloch is the beginning of technical neuron networks
2. Quiet years
3. In 1980s, the Renaissance ended the â€œquiet yearsâ€ , and a lot of neural paradigms were published

- MLP
- BP
- RBF
- ...

### Application

Neural networks and neural approaches can be found in a lot of  applications (academic and industrial):

- Character recognition  
- Signature verification  
- Face recognition 
- ...

Mainly 2 application areas:

1. Function approximation, Regression: the NN has the task to represent a goal  function (mostly continuous) as precisely as possible. 
2. Pattern recognition, Classification: the NN is mapping the input to a discrete set  of goal values. E.g.:binary classification  {O,I}, {-1,+1}, {OK, not OK}  or 1 out of M classification {0,1,2, â€¦ ,9}, {A,B,C, â€¦ Z}, ... 

### Three paradigms of learning

1. Supervised learning

The network learns from labeled examples: input-output pairs.

1. Reinforcement learning

The network learns by <b>trial and error</b>, guided by rewards.

1. Unsupervised learning

The network learns patterns <b>without labeled outputs</b>.

## Technical neural network paradigms 

### Neuron by McCulloch & Pitts 1943

Neuron model as logical binary switching elements with 2 states 0 and 1 

Input: a string of binary and each value has a weight of +1/-1

Output: sum up the wighted value, and compare it with a threshold value, if it's bigger then the neuron fires else remains calm

These neurons working together can implement the AND /NOT function, thus can realise all boolean functions

<img src="/assets/J8BcbMjXJorNgjxWTvicZnvZnEU.png" src-width="1166" src-height="466" align="center"/>

This model is the first mathematical model of a neuron. It is a binary threshold unit and has been the basic element for a lot of developments:

- Rosenblatt's Perzeptron (1957-1958
- ADALINE (1960), MADALINE: Widrow & Hoff  
- Lernmatrix: K.Steinbuch, (1961)  
- Perzeptrons (1963-1969
- ...

### Hebbian learning rule 1949

This first answers the question of how biological neural networks can learn or adapt.

He postulated the Hebbian learning rule that synaptic efficiency will increase if the pre-synatic side and the post-synatic side will be active at the same time window.

This rule is still the basis of most learning paradigms we have today.

> Cells that fire together, wire together.

$Î”w_{nm}=hâ‹…y_mâ‹…x_n$ 

- h is learning rate  
- $Î”w_{nm}$is the change of weigh between neuron m and n

### Perceptron, Frank Rosenblatt 1958

Inspired by the <b>structure of the biological retinaã€è§†ç½‘è†œã€‘</b>

Rosenblatt's perceptron is a layered structure 

- an array of sensors
- an intermediate layer of simple feature detectors(threshold units) 
- an output layer of threshold-units

It is able to perform <b>complex classification tasks</b>, and demonstrate biologycal properties such as generalization, lesion  tolerance, and learning.

The multi-layer perceptron structure is still basis for a lot of neural network paradigms today.

Rosenblatt æ„ŸçŸ¥å™¨ï¼ˆ1958ï¼‰ï¼šä¸‰å±‚ç»“æ„ï¼Œå…·å¤‡å­¦ä¹ ä¸æ³›åŒ–èƒ½åŠ›

Widrow & Hoff çš„ ADALINEï¼ˆ1960ï¼‰ï¼šçº¿æ€§ç¥ç»å…ƒ + LMS å­¦ä¹ è§„åˆ™

### Perzeptron

å¯å®ç°çº¿æ€§å¯åˆ†çš„å¸ƒå°”å‡½æ•°ï¼ˆå¦‚ AND, ORï¼‰

æ— æ³•å®ç° XOR â†’ éœ€è¦å¤šå±‚ç»“æ„

å¼•å…¥ BIAS æ¦‚å¿µç®€åŒ–é˜ˆå€¼å¤„ç†

æ„ŸçŸ¥å™¨å¯å°†è¾“å…¥ç©ºé—´æ˜ å°„ä¸ºè¾“å‡ºç©ºé—´ï¼Œå®ç°åˆ†ç±»æˆ–å‡½æ•°é€¼è¿‘

[Exercises sheet 1](/DyrnwQs57iVgXkkXv06cIEjKnkf/JKUfwrJdiivoFdkfgChcUoOknPc)

# L2

> Use these prompts to test your memory of the derivation.
> 1. <b>The Goal:</b> What is the "Error Function" $E$? How do we get the "Global Error" $G$? 
> 2. <b>The Method:</b> What is the "Error Surface"? What is "Gradient Descent"? 
> 3. <b>The Formula:</b> The gradient gives the <em>steepest ascent</em>. What formula do we use to <em>descend</em>? What does the $\eta$ (eta) symbol represent? 
> 4. <b>The Tool:</b> What mathematical rule is the key to solving $\frac{\partial E}{\partial w}$ for a multi-layer network?
> 5. <b>Output Layer (m):</b>
    - To find $\Delta w_{hm}$, we must solve $\frac{\partial E}{\partial w_{hm}}$.
    - What is the 3-part chain to solve this? ($\dots \rightarrow \dots \rightarrow \dots \rightarrow w_{hm}$)
    - What is the final $\delta$-rule for an output neuron, $\delta_m$? 
> 6. <b>Hidden Layer (h):</b>
    - To find $\Delta w_{gh}$, we must solve $\frac{\partial E}{\partial w_{gh}}$.
    - What is the 4-part chain to solve this? ($\dots \rightarrow \dots \rightarrow \dots \rightarrow \dots \rightarrow w_{gh}$)
    - What is the <em>hardest part</em> of this chain? (Hint: How does $out_h$ affect $E$?) 
    - How do we solve this "hard part"? What does the $\sum$ (summation) represent? 
    - What is the final $\delta$-rule for a hidden neuron, $\delta_h$? 
> 7. <b>The Algorithm:</b> What are the 7 steps of the Backpropagation algorithm? (Acronym: <b>I.P.F.C.B.U.R.</b>) 

## The Goal: Error Minimization

- Error Function (per pattern p): We measure the "mistake" for a single training pattern p using a sum-of-squares error function. The $\frac{1}{2}$ is a mathematical convenience that simplifies the derivative.
    $$^{p}E = \frac{1}{2}\sum_{m=1}^{M}(^{p}\hat{y}_{m} - ^{p}y_{m})^{2}$$
    - The P is for a specific, single training pattern $p$
    - $^{p}\hat{y}_{m}$is the teacher value, the desired output for pattern $p$
    - M is the number of neurons in the output layer.  So the network's output is an M-dimensional vector, which means the real output and desired output is like [$y_1,y_2,y_3...y_M$] ,for each neuron in output layer, neuron $i$ produces a $y_i$ 

- <b>Global Error (G):</b> The total error across all `P` training patterns is the sum of all individual pattern errors. 

$$G = \sum_{p=1}^{P} {}^{p}E$$

- <b>Error Surface:</b> This is the central concept. The Global Error $G$ can be visualized as a high-dimensional landscape or "surface," where the "altitude" at any point is determined by the current values of all the weights $W$ in the network. 
- <b>The Objective:</b> Our goal is to find the set of weights $W$ that corresponds to the lowest possible altitude (a "minimum") on this error surface. 

## The Method: Gradient Descent

- <b>"Skiing Downhill":</b> We find the minimum of the error surface by "skiing downhill" from our starting position. 
- <b>The Gradient (</b>$\nabla$<b>):</b> The gradient ($\nabla_w G$) is a vector that points in the direction of the <b>steepest uphill slope</b> at our current position. 
- <b>The Learning Rule:</b> To go <em>downhill</em>, we must take a step in the <b>opposite</b> (negative) direction of the gradient. 

$$\Delta w \sim -\nabla_{w}G$$

- <b>The Learning Rate (</b>$\eta$<b>):</b> We introduce a constant $\eta$ (eta), the <b>learning rate</b>, to control how large our step ($\Delta w$) is. This changes the "proportional to" ($\sim$) into an equals sign. 

$$\Delta w = -\eta \nabla_{w}G$$

- <b>For a single weight </b>$w_s$<b>:</b> The change for one weight is proportional to the negative partial derivative of the error with respect to that weight. 

$$\Delta w_{s} = -\eta \frac{\partial^{p}E}{\partial w_{s}}$$

# 
## The Backpropagation Formulas (The $\delta$-Rule)

We derived the solution for $\frac{\partial^{p}E}{\partial w_{s}}$ by using the chain rule. This process led to a clean, two-part learning rule:

### . The General Update Rule (The $\delta$-Rule):

The change for any weight $w_{ij}$ (connecting neuron $i$ to neuron $j$) is:

$$\Delta w_{ij} = \eta \cdot \delta_j \cdot \tilde{out}_i$$

- $\eta$ = Learning Rate
- $\delta_j$ = The "error signal" of the <b>destination</b> neuron $j$.
- $\tilde{out}_i$ = The output of the <b>source</b> neuron $i$, also the input of neuron $j$

> $$\Delta w_{ij} = -\eta \frac{\partial ^{p}E }{\partial w_{ij}}$$
> 1. First chain rule:
> $$\Delta w_{ij} = -\eta(\frac{\partial ^p E}{\partial ^p net_j}) \cdot (\frac{\partial ^p net_j}{\partial w_{ij}})$$
> $$=-\eta(\frac{\partial ^p E}{\partial ^p net_j})\cdot ^pout_i$$
> $net_j$is calculated by the first step: weighted sum
> Then it will be passed to a transfer function
> $$net_j = \sum_{k}{}w_{kj}\cdot out_k$$
> Only when $k=i$, the item is related to $w_{ij}$,so :
> $$\frac{\partial ^p net_j}{\partial w_{ij}} = \frac{d (w_{ij}
\cdot out_i)}{d w_{ij}}=^pout_i$$
> 1. using $\delta = -\frac{\partial ^p E}{\partial ^p net_j}$, so we have the $\delta$-rule

### . The Error Signal ($\delta$) Formulas:

The key to the algorithm is calculating $\delta_j$. The formula is different depending on where the neuron is.

#### For an OUTPUT Layer Neuron ($m$):

We can calculate its error $\delta_m$ by directly comparing its output $y_m$ to the "teacher" value $\hat{y}_m$.

$$\delta_m = (^{p}\hat{y}_m - ^{p}y_m) \cdot f'(^{p}net_m)$$

- $(\hat{y}_m - y_m)$ = The actual error (teacher - output). 
- $f'(net_m)$ = The derivative of the activation function at that neuron. 

> Start from $\delta_m = -\frac{\partial ^pE}{\partial ^p net_m}$
> 1. First chain rule
> $$\frac{\partial ^pE}{\partial ^p net_m}  = \frac{\partial ^pE}{\partial y_m} \cdot \frac{\partial y_m}{\partial net_m}$$
> 1. $y_m = f(net)$ so $\frac{\partial y_m}{\partial net_m} = f'(net_m)$
> 2. From the definition of $^pE = \frac{1}{2} \cdot \sum_{j=1}^{M} (^p \hat y_j-^py_j)^2$,only when $j=m$ , it has meaning, so 
> $$\frac{\partial ^pE}{\partial y_m} = \frac{d \frac 12 \cdot (^p \hat y_m-^p y_m)^2}{dy_m}=^p \hat y_m-^p y_m$$

#### For a HIDDEN Layer Neuron ($h$):

We have no "teacher" value for a hidden neuron. We must calculate its error $\delta_h$ by looking at the errors from the next layer ($\delta_k$) and propagating them backward.

$$\delta_h = \left( \sum_{k=1}^{K} \delta_k \cdot w_{hk} \right) \cdot f'(^{p}net_h)$$

- $\sum_{k=1}^{K} \delta_k \cdot w_{hk}$ = The <b>sum</b> of all error signals $\delta_k$ from the next layer, each multiplied by the weight $w_{hk}$ that connects $h$ to $k$. 
- $f'(net_h)$ = The derivative of the transfer function at that hidden neuron. 

> We start from $\delta_h = -\frac{\partial^{p}E}{\partial^{p}net_h}$
> 1. Using chain rule
    $$\frac{\partial^{p}E}{\partial^{p}net_h} = \frac{\partial^{p}E}{\partial^{p}out_h} \cdot \frac{\partial^{p}out_h}{\partial^{p}net_h}$$
> 2. $$\frac{\partial^{p}out_h}{\partial^{p}net_h} = f'(net_h)$$
> 3. The output $out_h$ affects the total error $E$ <em>indirectly</em> because it is an input to <em>all</em> $K$ neurons in the <em>next</em> layer. We must sum up all these contributions.
    $$\frac{\partial^{p}E}{\partial out_h} = \sum_{k=1}^{K} \left( \frac{\partial^{p}E}{\partial net_k} \cdot \frac{\partial net_k}{\partial out_h} \right)$$
> 4. $$\frac{\partial net_k}{\partial out_h} = \frac {d (out_h\cdot w_{hk})}{d(out_h)}=w_{hk}$$
> so:
> $$\frac{\partial^{p}E}{\partial out_h} = \sum_{k=1}^{K} \left( \frac{\partial^{p}E}{\partial net_k} \cdot w_{hk} \right)$$
> 1. $\frac{\partial^{p}E}{\partial net_k}=\delta_k$ so:
> $$\delta_h = \left( \sum_{k=1}^{K} \delta_k \cdot w_{hk} \right) \cdot f'(^{p}net_h)$$

## The Full Backpropagation Algorithm (The 7 Steps)

This is the complete learning loop that combines all the concepts above:

1. <b>I</b>nitialization: Set all weights $w$ to small, random values.
2. <b>P</b>ick a pattern: Select one training sample ($^{p}X$, $^{p}\hat{Y}$) from your dataset.
3. <b>F</b>orward: Feed $^{p}X$ into the network and calculate all $net$ and $out$ values, layer by layer, until you get the final output $^{p}Y$.
4. <b>C</b>ompare: Use the final output $^{p}Y$ and the teacher $^{p}\hat{Y}$ to...
5. <b>B</b>ackward:
    - ...calculate the error signals $\delta_m$ for the <b>output layer</b>.
    - ...use the $\delta_m$ signals to calculate the error signals $\delta_h$ for the <b>hidden layer(s)</b>.

6. <b>U</b>pdate: Use the $\delta$ signals from all layers to calculate $\Delta w_{ij}$ for all weights, and add this change to them: $w_{ij} = w_{ij} + \Delta w_{ij}$.
7. <b>R</b>eady?: Check if your "stop criterion" is met (e.g., global error $G$ is low enough, or you've run for 1000 epochs).
    - <b>No:</b> Go back to Step 2 and pick a new pattern.
    - <b>Yes:</b> Stop. The network is trained.

# L4

## initialization

1. 
- Global minimum 
- suboptimal local  minimum (wrong  valley )

1. Symmetry breaking 

Can't have same weights

set initial weight distributed -0.1~0.1 /0.5

## EFfect of the error surface 

steep valley (small change have big effect )

Isolated point (smooth function so hard to find this point )

FLat plateau (gradient close to 0, got stuck)

Fissured surfaces

### Local minima

Some way to deal with local minima

1. several runs/muti-start method (different starting points )
2. Large learning rate (step out local minima)
3. 
## Learning Curve

 The learning curve is the graph showing the development 
of the error (the single error E, or the global error G) with 
respect to time, or the number of patterns presented.

1. smoothly decreasing
2. stepped
3. noisy/ jittery because single step mode

# BP Extension

## Momentum

## Weight Decay

## Pruning

# L5

### P=K

Let's break down exactly how we go from a single messy summation formula to that clean, elegant matrix equation on <b>Page 55</b>.

1. The Starting Point: One Pattern, One Output

First, look at the basic formula for a single pattern $p$ and a single output neuron (simplification $M=1$) on <b>Page 47</b>.

The network output $y$ for pattern $p$ is the sum of all the weighted RBF responses:

$$^p\hat{y} = \sum_{k=1}^{K} w_{k1} \cdot ^pr_k$$

- $^p\hat{y}$ is the target value for pattern $p$.
- $w_{k1}$ is the weight for the $k$-th RBF neuron.
- $^pr_k$ is the response of the $k$-th RBF neuron to pattern $p$.

1. Writing it Out for ALL Patterns (The "Long" Way)

Now, imagine we have $P$ different training patterns. We need this equation to be true for <b>every single one</b> of them.

As shown on <b>Pages 50â€“52</b>, we can write them as a list of separate linear equations:

- <b>Pattern 1:</b> $\hat{y}_1 = r_{1,1}w_1 + r_{1,2}w_2 + \dots + r_{1,K}w_K$
- <b>Pattern 2:</b> $\hat{y}_2 = r_{2,1}w_1 + r_{2,2}w_2 + \dots + r_{2,K}w_K$
- ...
- <b>Pattern P:</b> $\hat{y}_P = r_{P,1}w_1 + r_{P,2}w_2 + \dots + r_{P,K}w_K$

Notice a pattern? We have a column of $\hat{y}$'s on the left, a block of $r$'s in the middle, and a repeating column of $w$'s.

1. The Matrix Transformation (Page 55)

Mathematicians are lazy (efficient!). Instead of writing that long list, they pack the numbers into grids (matrices) (1).

- The Target Vector ($\hat{Y}$):

We stack all the target values into a single column vector.

$$\hat{Y} = \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_P \end{bmatrix}$$

- The Weights Vector ($W$):

We stack all the unknown weights into a single column vector.

$$W = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_K \end{bmatrix}$$

The RBF Matrix ($R$):

This is the most important part. We take all the $r$ values (the response of every neuron to every pattern) and arrange them into a grid.

$$R = \begin{bmatrix} r_{1,1} & r_{1,2} & \dots & r_{1,K} \\ r_{2,1} & r_{2,2} & \dots & r_{2,K} \\ \vdots & \vdots & \ddots & \vdots \\ r_{P,1} & r_{P,2} & \dots & r_{P,K} \end{bmatrix}$$

# 
1. The Result

Now, using the rules of matrix multiplication (row times column), that massive list of equations collapses into:

$$\hat{Y} = R \cdot W$$

This is the equation shown on <b>Page 55</b>.

The Catch:

For the approximation case ($P > K$), as noted on Page 60, the matrix $R$ is a tall rectangle (more rows than columns).

According to Page 61, since the standard inverse $R^{-1}$ does not exist for rectangular matrices, what specific "extended" inverse do we use instead? (2)

### P&gt;k

You nailed it again! ğŸ”¨ It is indeed the <b>Moore-Penrose Pseudo-Inverse</b> (often called MPPI or just the pseudo-inverse).

This is the mathematical "magic trick" that lets us solve the equation even when the matrix isn't square.

Here is exactly how it works, according to <b>Page 62</b>:

1. The Formula for $R^+$

Since we can't invert the rectangular matrix $R$ directly, we multiply it by its transpose ($R^T$) to <em>force</em> it to become square.

- $R$ is size $(P \times K)$
- $R^T$ is size $(K \times P)$
- Multiplying $R^T \cdot R$ creates a square matrix of size $(K \times K)$.

Because this new matrix is square, we <b>can</b> invert it (usually). The formula for the pseudo-inverse $R^+$ basically packages this whole operation into one symbol:

$$R^+ = (R^T \cdot R)^{-1} \cdot R^T$$

1. Calculating the Weights (Page 64)

Now we just plug $R^+$ back into our original weight equation:

$$W = R^+ \cdot \hat{Y}$$

This gives us the <b>optimal</b> weights. When we say "optimal" here, we mean it minimizes the squared error (Least Mean Squares) between what the network outputs and what the teacher wants.

---

So, we have two ways to train this network:

1. <b>Analytic Solution (MPPI):</b> Do all this matrix math and get the exact answer in one shot.
2. <b>Iterative Learning (Gradient Descent)</b><b>:</b> Slowly adjust the weights step-by-step (like in standard Backpropagation), as shown on <b>Page 67</b>.ã€faster,preciser but no direct math solution and the same problem as MLPã€‘

---

Pros and cons

- Advantages: a mathematically exact calculation and has constant and known time requirements (you know exactly how many operations the matrix math will take).
- Disadvantages: computationally expensive (calculating inverses for huge matrices takes a lot of power) and numerically unstable for large matrices.

# ! BPè¿˜æ˜¯æœ‰ä¸å¤ªä¼šçš„

æ•°å­¦æ¨å¯¼å†æ¥ä¸€é

# L8 ART & Hopfield

## ART

### Definition and types

definition of ART : a family of neural network paradigms, developed by  G.Carpenter  and S.Grossberg since 1976.

> ART; Adaptive Resonance Theory is a family of neural  network paradigms that have been developed by G.Carpenter  and S.Grossberg since 1976.

Members of ART network family:

- ART 1 1976 and 1987,  
- ART 2 1987, 
- ART 2A-2C 1991,  
- ART 3 1990,  
- ART MAP 1991, Fuzzy ART 1991,  
- Fuzzy ART MAP 1992.  

### motivation and ability

Motivation for ART-1: aim to overcome the <b>stability-plasticity dilemma</b>( competitive learning paradigms has such problem)

ability of ART networks:ã€æ— ç›‘ç£ã€‘

- Pattern recognitionã€classificationä»»åŠ¡ï¼Œä½†æ˜¯ä¸æ˜¯ç›‘ç£å¼ ART ä¼šæŠŠè¾“å…¥æ¨¡å¼æ˜ å°„åˆ° F2 å±‚çš„æŸä¸ªç¥ç»å…ƒï¼ˆç±»åˆ«åŸå‹ï¼‰ã€‚ã€‘
- Content-addressable memory
- <b>Unsupervised</b> learningã€ART1 å®Œå…¨ä¸éœ€è¦æ ‡ç­¾ã€‚ å®ƒé€šè¿‡â€œå…±æŒ¯ï¼ˆresonanceï¼‰â€æœºåˆ¶å†³å®šæ˜¯å¦åˆ›å»ºæ–°ç±»åˆ«ã€‚ã€‘
- Clustering ã€ART çš„ F2 å±‚ç¥ç»å…ƒæœ¬è´¨ä¸Šå°±æ˜¯èšç±»ä¸­å¿ƒã€‚ æ¯ä¸ª F2 ç¥ç»å…ƒä»£è¡¨ä¸€ä¸ª clusterã€‚ã€‘

### ART-1

- 2 layers F1 F2
    - F1 Comparison layer
        - Contains N neurons
        - Receives binary input vector X(dim = N)
        - Produces comparison signals $s_n$
    - F2 Recognition layer
        - Contains M neurons, each neuron represents a category
        - Produces recognition responses $r_m$
        - Outputs a oneâ€‘outâ€‘ofâ€‘M winner vector Y(F2 ä½¿ç”¨ Winner-Takes-Allï¼ˆWTAï¼‰ï¼šå“åº”æœ€å¤§çš„ç¥ç»å…ƒ = 1;å…¶ä»–æ‰€æœ‰ç¥ç»å…ƒ = 0)

- 2 weight matrices
    - Forward weights W
        - Realâ€‘valued $W_{nm}$
        - Carry signals from F1 â†’ F2
    - Backward weights V
        - Binary $V_{nm}$
        - Project from F2 â†’ F1

- 2 layers represents short-term memory STM or working-memory
- ART-1  is a binary network(input 0/1)

<img src="/assets/SPgXbb0JgooIkexVfBfcukSAnNv.png" src-width="934" src-height="398" align="center"/>

procedureï¼š

1. F1 receive binary input $X_n$
    1. Each F1 neuron n receives one component xn
    2. No transformation is applied
    3. output:  $s_n$ =  $X_n$

2. F1 connect with F2 via W
    1. W is realâ€‘valued, size NÃ—M
    2. Each neuron connect with all neurons of F2 with weight  matrix W

3. F2 receive from F1
    1. F2 neuron j receives  $x_n*w_{nj}$from F1 neuron n
    2. Each neuron j in F2 will receive $r_j=âˆ‘_{n=1}^N x_n w_{nj}$
    3. Select one winner that is largest, output $Y_m$ only one dim is 1 else 0ã€WTA winner takes allã€‘

<img src="/assets/Jm4RbQTN0oEWNqxzmkNcVLyTnyd.png" src-width="938" src-height="430" align="center"/>

1. F2 project back to F1 via V
    1. F2 send back to F1 by weight matrix V(binary)
    2. For F2 neuron j , send out  $Y_j$(0 or 1) *  Vji to neuron i in F1ã€ç›¸å½“äº é€‰ä¸­çš„é‚£ä¸ªF2 neuronå‘é€å‡ºå»çš„æ‰æ˜¯æœ‰æ•ˆçš„ã€‘

2. F1 receive returns from F2
    1. Each neuron will receive  $net_n$(dim = n)
    2. For neuron i  $net_i=âˆ‘_{m=1}^Mv_{mi}y_m$
    3. Only $y_j$ is 1 so   $net_i=v_{ji}$
    4. Backward projection å°† F2 èƒœè€…ç¥ç»å…ƒ j çš„ prototypeï¼ˆV çš„ç¬¬ j è¡Œï¼‰å®Œæ•´åœ°æŠ•å°„å› F1ï¼Œä½¿å¾— F1 æ”¶åˆ°çš„å‘é‡æ­£æ˜¯Vçš„ç¬¬jè¡Œ å°±æ˜¯WTAé‡Œé¢é‚£ä¸ªèƒœåˆ©è€…å¯¹åº”çš„prototype

3. F1 neuron computes new output 
    æ¯ä¸ª F1 ç¥ç»å…ƒ n æœ‰ä¸‰ä¸ªè¾“å…¥ï¼š
    1. è¾“å…¥å‘é‡xn
    2. ä» F2 è¿”å›çš„ç±»åˆ«æ¨¡ç‰ˆ $net_n=v_{jn}$
    3. æ§åˆ¶ä¿¡å· gain<b> </b>g1
    $$s_n =
\begin{cases}
1, & \text{if } x_n \cdot \text{net}_n = 1 \\
1, & \text{if } x_n \cdot g_1 = 1 \\
1, & \text{if } \text{net}_n \cdot g_1 = 1 \\
0, & \text{otherwise}
\end{cases}$$
    2/3 rule
    è§£é‡Šï¼š
    - <b>å¦‚æœè¾“å…¥å’Œæ¨¡æ¿éƒ½ä¸º 1 â†’ ä¿æŒ 1</b>
    - <b>å¦‚æœè¾“å…¥ä¸º 1 ä¸” g1 æ¿€æ´» â†’ 1</b>
    - <b>å¦‚æœæ¨¡æ¿ä¸º 1 ä¸” g1 æ¿€æ´» â†’ 1</b>
    - å¦åˆ™è¾“å‡º 0

4. New F1 output
    1. Send the new s' to F2
    2. Repite the cycle until resonance
    è¿™å°±æ˜¯æ‰€è°“çš„ <b>twoâ€‘third rule</b>ï¼ˆä¸‰ä¸ªæ¡ä»¶ä¸­æ»¡è¶³ä»»æ„ä¸€ä¸ªå³å¯ï¼‰ã€‚

## Hopfield

### åŠ¨æœºä¸ç§‘å­¦èƒŒæ™¯ (Motivation)

Hopfield ç½‘ç»œåœ¨ 1982 å¹´ç”± John J. Hopfield æå‡ºï¼Œå®ƒçš„å‡ºç°æ˜¯ç¥ç»ç½‘ç»œç ”ç©¶é‡æ–°å›åˆ°å›½é™…èˆå°çš„â€œå‘ä»¤æªâ€ (1)(1)(1)(1)ã€‚è®²ä¹‰æŒ‡å‡ºå…¶çµæ„Ÿæ¥æºäºä¸‰å¤§é¢†åŸŸï¼š

- <b>ç¥ç»ç”Ÿç‰©å­¦</b>ï¼šæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„äºŒå€¼å‘æ”¾çŠ¶æ€ï¼ˆFiring æˆ– Not Firingï¼‰ (2)(2)(2)(2)ã€‚
- <b>è®¡ç®—æœºç§‘å­¦</b>ï¼šæ—¨åœ¨å®ç°<b>å†…å®¹å¯å¯»å€å­˜å‚¨ï¼ˆContent-addressable memory, CAMï¼‰</b> (3)(3)(3)(3)ã€‚
- <b>ç»Ÿè®¡ç‰©ç†å­¦</b>ï¼šå€Ÿé‰´äº†ç£æ€§ææ–™ï¼ˆè‡ªæ—‹ç»ç’ƒ Spin-Glassesï¼‰çš„ç‰¹æ€§ï¼Œåˆ©ç”¨åŸå­è‡ªæ—‹ï¼ˆåŒ—æ/å—æï¼‰çš„äºŒå€¼çŠ¶æ€è¿›è¡Œä¿¡æ¯å¤„ç† (4)(4)(4)(4)ã€‚

---

### æ ¸å¿ƒä»»åŠ¡ (Task)

æ–‡æ¡£æ˜ç¡®äº† Hopfield ç½‘ç»œçš„ä¸¤ä¸ªä¸»è¦ç”¨é€”ï¼š

1. <b>è”æƒ³è®°å¿†ï¼ˆAssociative Memoryï¼‰</b>ï¼š
    - è¿™æ˜¯ä¸€ç§â€œä»¥å†…å®¹ä¸ºç´¢å¼•â€çš„å­˜å‚¨æ–¹å¼ (5)ã€‚
    - å³ä½¿æä¾›çš„æ˜¯åŸå§‹ä¿¡æ¯çš„ç¢ç‰‡æˆ–æ®‹ç¼ºéƒ¨åˆ†ï¼Œç³»ç»Ÿä¹Ÿèƒ½æ£€ç´¢å¹¶è¿˜åŸå‡ºå®Œæ•´çš„åŸå§‹ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼šè¾“å…¥å›¾ç‰‡çš„ä¸€éƒ¨åˆ†ï¼Œç³»ç»Ÿæ¢å¤å®Œæ•´å›¾ç‰‡ï¼‰ (6)ã€‚

2. <b>è§£å†³ä¼˜åŒ–é—®é¢˜</b>ï¼šåˆ©ç”¨å…¶èƒ½é‡æœ€å°åŒ–çš„ç‰¹æ€§æ¥å¯»æ‰¾é—®é¢˜çš„æœ€ä¼˜è§£ (7)ã€‚

---

### æ•°å­¦ç»“æ„ (Structure)

è®²ä¹‰è¯¦ç»†å®šä¹‰äº†ç¡¬ä»¶å’Œå‚æ•°çš„è®¾ç½®è§„åˆ™ï¼š

- <b>ç¥ç»å…ƒå®šä¹‰</b>ï¼š
    - ç”± $K$ ä¸ªå®Œå…¨ç›¸åŒçš„äºŒå€¼ç¥ç»å…ƒç»„æˆã€‚
    - çŠ¶æ€ $x_k \in \{-1, +1\}$ ã€‚
    - æ•´ä¸ªç½‘ç»œçš„çŠ¶æ€æ˜¯ä¸€ä¸ª $K$ ç»´äºŒè¿›åˆ¶å‘é‡ $X$ (10)ã€‚

- <b>æƒé‡çŸ©é˜µ </b>$W$<b> çš„ä¸‰å¤§é“å¾‹</b>ï¼š
    - <b>å…¨è¿æ¥</b>ï¼šæ¯ä¸ªç¥ç»å…ƒ $i$ ä¸é™¤è‡ªå·±å¤–çš„æ‰€æœ‰ç¥ç»å…ƒ $j$ ç›¸è¿ ã€‚
    - <b>å¯¹ç§°æ€§</b>ï¼šå¿…é¡»æ»¡è¶³ $w_{ij} = w_{ji}$ ã€‚
    - <b>æ— è‡ªåé¦ˆ</b>ï¼šå¯¹è§’çº¿å…ƒç´ å¿…é¡»ä¸ºé›¶ï¼Œå³ $w_{ii} = 0$ ã€‚

---

### åŠ¨åŠ›å­¦ä¸æ›´æ–°è§„åˆ™ (Functionality: Dynamics)

è¿™æ˜¯ç½‘ç»œå¦‚ä½•â€œè¿è¡Œâ€çš„æ ¸å¿ƒé€»è¾‘ã€‚

#### æ›´æ–°å…¬å¼

æ¯ä¸ªç¥ç»å…ƒ $k$ çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ $x_k(t+1)$ å–å†³äºåŠ æƒæ€»å’Œä¸é˜ˆå€¼ $\theta_k$ çš„æ¯”è¾ƒ ï¼š

- <b>æ ‡å‡†è§„åˆ™</b>ï¼šå¦‚æœåŠ æƒå’Œ $z > \theta_k$ï¼Œåˆ™ $x_k = +1$ï¼›å¦åˆ™ä¸º $-1$ ã€‚
- <b>å˜ä½“è§„åˆ™</b>ï¼šå¦‚æœåŠ æƒå’Œæ­£å¥½ç­‰äºé˜ˆå€¼$\theta_k$ï¼Œç¥ç»å…ƒä¿æŒå‰ä¸€ä¸ªçŠ¶æ€ $x_k(t)$ ä¸å˜ ã€‚

#### å¼‚æ­¥æ›´æ–° (Asynchronous Update) â€”â€” æ–‡æ¡£å¼ºè°ƒè¿™æ˜¯æ ‡å‡†æ“ä½œ

- <b>æœºåˆ¶</b>ï¼šåœ¨æ¯ä¸€ä¸ªæ›´æ–°æ­¥éª¤ä¸­ï¼Œ<b>åªé€‰æ‹©ä¸€ä¸ª</b>ç¥ç»å…ƒ $j$ è¿›è¡ŒçŠ¶æ€æ›´æ–°ã€‚
- <b>åé¦ˆæ•ˆåº”</b>ï¼šä¸‹ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—ä¼šâ€œçœ‹åˆ°â€è¿™ä¸ªå·²ç»æ”¹å˜çš„çŠ¶æ€ï¼Œè¿™ç§åé¦ˆæ¨åŠ¨äº†ç³»ç»Ÿçš„æ¼”åŒ– ã€‚
- <b>é¡ºåº</b>ï¼šç¥ç»å…ƒçš„æ›´æ–°åºåˆ—åº”è¯¥æ˜¯éšæœºçš„ï¼Œä¸”æ¯ä¸ªç¥ç»å…ƒåœ¨æ¯ä¸€è½®ï¼ˆæ—¶é—´æ­¥ $t$ï¼‰ä¸­åº”æ°å¥½è¢«é€‰ä¸­ä¸€æ¬¡ ã€‚

#### èƒ½é‡å‡½æ•° (Energy Function)

æ–‡æ¡£å¼•å…¥äº†èƒ½é‡ $E$ çš„æ¦‚å¿µæ¥è¯æ˜æ”¶æ•›æ€§ 212121ï¼š

$$E = -\frac{1}{2}\sum_{i}^{K}\sum_{j}^{K}w_{ij}x_{i}x_{j} + \sum_{k}^{K}\theta_{k}x_{k}$$

- <b>æ”¶æ•›è¯æ˜ç‚¹</b>ï¼šåœ¨å¯¹ç§°æƒé‡å’Œå¼‚æ­¥æ›´æ–°ä¸‹ï¼Œæ¯ä¸€æ¬¡æ›´æ–°éƒ½ä¼šä½¿ $E$ å‡å°æˆ–ä¿æŒä¸å˜ã€‚
- <b>æœ€ç»ˆæ€</b>ï¼šç³»ç»Ÿæœ€ç»ˆä¼šåœåœ¨ä¸€ä¸ªç¨³å®šçš„æœ€ç»ˆæ¨¡å¼ $X^*$ ä¸Š ã€‚

---

### å­¦ä¹ ä¸è®­ç»ƒ (Learning)

å­¦ä¹ çš„è¿‡ç¨‹å°±æ˜¯é€šè¿‡è®¾ç½®æƒé‡ $w_{ij}$ï¼Œåœ¨èƒ½é‡æ›²é¢ä¸Šâ€œæŒ–å‘â€ï¼ˆåˆ›é€ æå°å€¼ç‚¹ï¼‰ï¼Œä½¿è®­ç»ƒæ¨¡å¼å¤„äºå‘åº• ã€‚

1. <b>å­˜å‚¨å•ä¸ªæ¨¡å¼</b>ï¼š
    - <b>é€»è¾‘</b>ï¼šå¦‚æœä¸¤ä¸ªç¥ç»å…ƒåœ¨æ¨¡å¼ä¸­çŠ¶æ€ç›¸åŒï¼Œæƒé‡åº”ä¸ºæ­£ï¼›ä¸åŒåˆ™ä¸ºè´Ÿ ã€‚
    - <b>å…¬å¼</b>ï¼š$w_{ik} = x_i \cdot x_k$ã€‚

2. <b>å­˜å‚¨ </b>$P$<b> ä¸ªæ¨¡å¼</b>ï¼š
    - <b>å…¬å¼</b>ï¼š$w_{ik} = \sum_{p=1}^{P} x_i^p \cdot x_k^p$ ã€‚
    - <b>å®ç°æ–¹å¼</b>ï¼šå°† $P$ ä¸ªå•æ¨¡å¼ç”Ÿæˆçš„çŸ©é˜µè¿›è¡Œç´¯åŠ ã€‚

---

### å±€é™æ€§ä¸åº”ç”¨ (Limitations & Applications)

- <b>å­˜å‚¨å®¹é‡é™åˆ¶</b>ï¼šæœ€å¤§å­˜å‚¨æ•°é‡ $P_{max} \approx 0.138 \times K$ã€‚
- <b>æ“ä½œæ¨¡å¼</b>ï¼š
    - <b>è‡ªåŠ¨è”æƒ³æ¨¡å¼ (Autoassociator)</b>ï¼šå°†æ¨¡å¼ä¸è‡ªèº«å…³è”ï¼ˆå¯¹ç§°çŸ©é˜µï¼‰ (30)ã€‚
    - <b>å¼‚è”æƒ³æ¨¡å¼ (Heteroassociator)</b>ï¼šå°†æ¨¡å¼ $p$ ä¸æ¨¡å¼ $q$ å…³è”ï¼ˆçŸ©é˜µä¸å†å¯¹ç§°ï¼‰ï¼Œå¯ç”¨äºå­¦ä¹ åºåˆ—ã€‚

- <b>å®é™…åº”ç”¨</b>ï¼šæ¨¡å¼è¯†åˆ«ã€å»å™ªã€å­—ç¬¦è¯†åˆ«ï¼Œç”šè‡³æ˜¯é‡å­è®¡ç®—çš„ç›¸å…³ç ”ç©¶ã€‚
- <b>ç¡¬ä»¶å®ç°</b>ï¼šåŒ…æ‹¬ 80 å¹´ä»£çš„æ•°å­—ç¨€ç–ç¼–ç ã€è‡ªæ—‹ç»ç’ƒæ¨¡æ‹Ÿä»¥åŠå…‰å­¦å®ç°æ–¹æ¡ˆã€‚

# L9 Recurrent MLPs & Neocognitron

## Recurrent MLPs

### Motivation

1. ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦â€œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRecurrent Networksï¼‰â€ã€‚

ä¸‰ç§å…¸å‹çš„ç¥ç»ç½‘ç»œæ‹“æ‰‘ç»“æ„

è®²ä¹‰æƒ³è¡¨è¾¾çš„æ ¸å¿ƒï¼š

> <b>ç°å®ä¸–ç•Œå¾ˆå¤šä»»åŠ¡æ˜¯æ—¶é—´ç›¸å…³çš„ï¼ˆåºåˆ—ï¼‰ï¼Œå‰é¦ˆç½‘ç»œæ— æ³•å¤„ç†â€œè¿‡å»çš„ä¿¡æ¯â€ï¼Œæ‰€ä»¥éœ€è¦ Recurrent Networksã€‚</b>

1. ä»€ä¹ˆæ˜¯ Recurrent MLPï¼Ÿ

è®²ä¹‰è¯´ï¼š

> ä¸€ç§ç‰¹æ®Šçš„é€’å½’ç½‘ç»œï¼Œå…¶æœ¬è´¨æ˜¯ MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰ï¼Œä½†åŠ å…¥äº†å°‘é‡æ˜¾å¼çš„åé¦ˆè¿æ¥ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼š

- ç»“æ„ä¸Šä»ç„¶æ˜¯ MLPï¼ˆè¾“å…¥å±‚ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚ï¼‰
- ä½†åŠ å…¥äº†ä»åå±‚è¿”å›å‰å±‚çš„è¿æ¥ï¼ˆå¸¦æ—¶é—´å»¶è¿Ÿï¼‰
- è¿™æ ·ç½‘ç»œå°±èƒ½â€œè®°ä½è¿‡å»â€

1. MLP structure å›é¡¾

MLP çš„ä¸‰ä¸ªå±‚æ¬¡ï¼š

1. Backpropagationï¼ˆå›é¡¾ï¼‰

æƒé‡æ›´æ–°å…¬å¼ï¼š

$$Î”w_ij=Î·â‹…Î´_jâ‹…out_i$$

Î´ çš„è®¡ç®—ï¼š

è®²ä¹‰åœ¨è¿™é‡Œå¤ä¹  BPï¼Œæ˜¯ä¸ºäº†åé¢è®²ï¼š

> <b>æ™®é€š BP ä¸èƒ½ç›´æ¥ç”¨äº Recurrent MLPï¼Œå› ä¸ºåé¦ˆä¼šå¯¼è‡´æ— é™åå‘ä¼ æ’­ã€‚</b>

### Recurrent MLP

1. Recurrent MLP çš„ç‰¹ç‚¹ï¼š
    - åœ¨æ™®é€š MLP çš„åŸºç¡€ä¸Š<b>åŠ ä¸Šåé¦ˆè¿æ¥feedback connections.</b>
    - åé¦ˆæ¥è‡ªâ€œåé¢çš„å±‚â€ â†’ â€œå‰é¢çš„å±‚â€
    - åé¦ˆä¿¡å·å¸¦æœ‰<b>æ—¶é—´å»¶è¿Ÿ</b>ï¼ˆt â†’ t+1ï¼‰ time delay 
    - è¿™æ ·ç½‘ç»œå°±èƒ½â€œè®°ä½è¿‡å»çš„è¾“å‡ºæˆ–éšè—çŠ¶æ€â€

2. ä¸ºä»€ä¹ˆè¦æœ‰æ—¶é—´å»¶è¿Ÿ Î”tï¼Ÿ

è®²ä¹‰è¯´ï¼š

> feedback lines typically have an explicit time delay â€¦ suspend the transmission by one time step t â†’ t+1

æ„æ€æ˜¯ï¼š

- åé¦ˆä¿¡å·ä¸æ˜¯ç«‹å³å›åˆ°ç½‘ç»œ
- è€Œæ˜¯å»¶è¿Ÿä¸€ä¸ªæ—¶é—´æ­¥
- è¿™æ ·ç½‘ç»œåœ¨ t+1 æ—¶åˆ»èƒ½çœ‹åˆ° t æ—¶åˆ»çš„è¾“å‡º

è¿™å°±æ˜¯ RNN çš„â€œè®°å¿†â€ã€‚

1. <b>ä¸¤ç§å…¸å‹çš„ Recurrent MLP ç»“æ„</b>

è®²ä¹‰è¯´ï¼š

> The two typical MLP architectures with feedback connections are:
<b>Jordan-Networks</b> and <b>Elman-Networks</b>

<b>They differ in the way the feedback connections are located.</b>

å®ƒä»¬çš„åŒºåˆ«åœ¨äºï¼š

- Jordanï¼šåé¦ˆæ¥è‡ª <b>è¾“å‡ºå±‚</b>
- Elmanï¼šåé¦ˆæ¥è‡ª <b>éšè—å±‚</b>

#### Jordan Networkâ€”â€”åé¦ˆæ¥è‡ªè¾“å‡ºå±‚

Jordan ç½‘ç»œç»“æ„ï¼š

- è¾“å‡ºå±‚ Y(t) â†’ åé¦ˆï¼ˆweighted with gï¼‰ â†’ <b>context neuronsï¼ˆä¸Šä¸‹æ–‡ç¥ç»å…ƒï¼‰</b>
- context neurons å­˜å‚¨è¿‡å»çš„è¾“å‡º Y(tâˆ’1)ï¼Œweighted with gï¼Œé€šå¸¸æ˜¯1
- context neurons çš„è¾“å‡ºä½œä¸º<b>é¢å¤–è¾“å…¥additional input </b>ä¼ ç»™éšè—å±‚ã€ç›¸å½“äºä¸è¾“å…¥å±‚ä½ç½®ä¸€æ · å¢å¼ºè¾“å…¥ ä¼ ç»™éšè—å±‚ã€‘
- context neuron çš„è¾“å‡ºä¹Ÿä¼šåé¦ˆç»™è‡ªå·±ï¼ˆè‡ªåé¦ˆï¼‰ï¼Œæƒé‡æ˜¯ Î»ï¼ˆé€šå¸¸æ˜¯0ï¼‰ä¸ä¿ç•™æ›´ä¹…çš„è®°å¿†ï¼Œåªè®°ä½ä¸Šä¸€æ—¶åˆ»ã€‚
- context neuron çš„è¾“å…¥:$c_m(t)=gâ‹…y_m(tâˆ’1)+Î»â‹…c_m(tâˆ’1)$
- context neuron çš„è¾“å‡º = å®ƒå½“å‰çš„å€¼ c(t)
- è¾“å…¥å±‚ â†’ éšè—å±‚çš„æƒé‡æ•°é‡æ˜¯ï¼š
    - (1+N+M)Ã—H
    - 1 = bias
    - N = è¾“å…¥ç»´åº¦
    - M = context neurons æ•°é‡
    - H = éšè—å±‚ç¥ç»å…ƒæ•°

```text
è¾“å…¥å±‚ï¼š X(t) = [x1(t), x2(t), ..., xN(t)]
contextï¼š C(t) = g * Y(t-1)<-|  
éšè—å±‚ï¼š H(t)                 |
è¾“å‡ºå±‚ï¼š Y(t)-----------------
```

#### Elman

1. Elman Network åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

> <b>Elman ç½‘ç»œ = æ™®é€š MLP + æŠŠéšè—å±‚çš„è¾“å‡º H(t) åé¦ˆåˆ°éšè—å±‚ï¼ˆé€šè¿‡ context neuronsï¼‰ï¼Œå¸¦æ—¶é—´å»¶è¿Ÿ Î”tã€‚</b>

ä¹Ÿå°±æ˜¯è¯´ï¼š

> <b>éšè—å±‚æœ‰è‡ªå·±çš„â€œè®°å¿†è¾“å…¥â€ã€‚</b>

1. æ¯ä¸€å±‚ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰éƒ½æœ‰ context neuronsï¼ˆä¸Šä¸‹æ–‡ç¥ç»å…ƒï¼‰ï¼š

- å®ƒä»¬å­˜å‚¨ä¸Šä¸€æ—¶åˆ»çš„éšè—å±‚è¾“å‡º H(tâˆ’1)
- ç„¶åæŠŠè¿™äº›å€¼ä½œä¸ºé¢å¤–è¾“å…¥é€å›éšè—å±‚
-  å¯¹éšè—å±‚æ¥è¯´ï¼š
    - éšè—å±‚æœ‰ H ä¸ªç¥ç»å…ƒ
    - å°±ä¼šæœ‰ H ä¸ª context neurons

å› ä¸ºæ¯ä¸ªéšè—ç¥ç»å…ƒéƒ½éœ€è¦ä¸€ä¸ªå¯¹åº”çš„è®°å¿†å•å…ƒã€‚

1. å’Œ Jordan ä¸åŒï¼š
    - <b>Elman æ²¡æœ‰ Î» è‡ªåé¦ˆ</b>
    - context neurons åªå­˜å‚¨ä¸Šä¸€æ—¶åˆ»çš„éšè—å±‚è¾“å‡º
    - $$c_h(t)=h_h(tâˆ’1)$$

Jordan æœ‰ Î»ï¼ŒElman æ²¡æœ‰ã€‚

#### <b>Jordan vs Elman</b>

#### Recurrent MLPsï¼ˆé€’å½’å¤šå±‚æ„ŸçŸ¥æœºï¼‰ çš„è¿è¡Œæ¨¡å¼ï¼ˆoperation modesï¼‰

> <b>Recurrent MLPsï¼ˆé€’å½’ MLPï¼‰å’Œæ™®é€šå‰é¦ˆ MLP ä¸ä¸€æ ·ï¼Œå®ƒä»¬çš„è¾“å‡ºä¸ä»…å–å†³äºå½“å‰è¾“å…¥ï¼Œè¿˜å–å†³äºè¿‡å»çš„è¾“å‡ºæˆ–çŠ¶æ€ï¼Œå› æ­¤å…·æœ‰â€œåŠ¨æ€è¡Œä¸ºâ€ã€‚dynamical behavior</b>

#### â€œ<b>dynamical behavior</b>â€ï¼Ÿ

å› ä¸ºï¼š

- æ™®é€š MLP æ˜¯é™æ€æ˜ å°„ï¼š
 [ $X(t) \rightarrow Y(t)$ ] æ¯æ¬¡è¾“å…¥ä¸€æ ·ï¼Œè¾“å‡ºå°±ä¸€æ ·ã€‚
- é€’å½’ MLP æ˜¯åŠ¨æ€æ˜ å°„ï¼š
 [ $Y(t) = f(X(t), Y(t-1))$ ] å³ä½¿è¾“å…¥ $X(t)$ ä¸å˜ï¼Œè¾“å‡º $Y(t)$ä¹Ÿå¯èƒ½å˜åŒ–ï¼Œå› ä¸ºå®ƒè¿˜ä¾èµ–è¿‡å»çš„è¾“å‡ºã€‚

---

1. Jordan ç½‘ç»œçš„ç‰¹æ®Šè¡Œä¸ºï¼ˆè®²ä¹‰é‡ç‚¹ï¼‰

è®²ä¹‰åå¤å¼ºè°ƒï¼š

> å¦‚æœæ˜¯ä¸€ä¸ª <b>Jordan ç½‘ç»œ</b>ï¼Œæƒé‡å›ºå®šï¼ˆä¸å­¦ä¹ ï¼‰ï¼Œè¾“å…¥ä¸å˜ $X(t+1) = X(t)$ï¼Œå®ƒä»ç„¶å¯ä»¥ç”Ÿæˆä¸€ä¸ª<b>æ— é™çš„è¾“å‡ºåºåˆ— </b>$Y(t)$ã€‚

è¿™è¯´æ˜ï¼š

- ç½‘ç»œæœ‰â€œè®°å¿†â€
- å³ä½¿è¾“å…¥ä¸å˜ï¼Œè¾“å‡ºä¹Ÿèƒ½å˜åŒ–
- Jordan ç½‘ç»œå¯ä»¥ç”Ÿæˆæ—¶é—´åºåˆ—ã€çŠ¶æ€åºåˆ—

---

1. åˆå§‹åŒ–è¿‡ç¨‹

è¿è¡Œ Jordan ç½‘ç»œæ—¶ï¼š

1. <b>åˆå§‹åŒ– context neurons</b>  
 æ¯”å¦‚ï¼š [ $c_1(t_0) = 0.0,\quad c_2(t_0) = 0.0$ ]
2. <b>è¾“å…¥ç¬¬ä¸€ä¸ªæ¨¡å¼</b>$X(t_0)$
 ç½‘ç»œè®¡ç®—å‡ºç¬¬ä¸€ä¸ªè¾“å‡º $Y(t_0)$
3. <b>è¾“å‡º </b>$Y(t_0)$<b> è¢«åé¦ˆåˆ° context neurons</b>  
 åœ¨ä¸‹ä¸€æ—¶åˆ» $t_1 = t_0 + 1$ï¼Œcontext neurons çš„å€¼å˜äº†
4. <b>éšè—å±‚æ¥æ”¶åˆ°æ–°çš„ context è¾“å…¥</b>  
 æ‰€ä»¥å³ä½¿ $X(t_1) = X(t_0)$ï¼Œè¾“å‡º $Y(t_1) \ne Y(t_0)$

è¿™å°±æ˜¯â€œåŠ¨æ€è¡Œä¸ºâ€ã€‚

---

#### å››ç§è¿è¡Œæ¨¡å¼ï¼ˆoperation modesï¼‰

è®²ä¹‰åé¢åˆ—å‡ºäº† <b>å››ç§è¿è¡Œæ¨¡å¼</b>ï¼Œéå¸¸é‡è¦ï¼š

Mode 1:  Input is constant, network output converges to a const. value.  

Mode 2:  Input is varying with time, network output converges.  

Mode 3:  Input is constant, network output <b>generates a sequence of states,  values or a continuous time series</b>.  

Mode 4:  Input is varying with time, network output generates a sequence of  states, values or a continuous time series.

### Train & Learning

1. <b>å¾ªç¯ MLPï¼ˆJordan / Elman ç½‘ç»œï¼‰å¿…é¡»å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ã€‚</b>

å› æ­¤ï¼š

- è¾“å…¥ X(t) å¿…é¡»æ˜¯éšæ—¶é—´å˜åŒ–çš„åºåˆ—  X(t) is <b>time  dependent data</b>; typically it is a complete<b> time series</b>
- å¦‚æœæ˜¯<b>ç›‘ç£å­¦ä¹ </b>ï¼Œæ•™å¸ˆä¿¡å· Y(t) ä¹Ÿå¿…é¡»æ˜¯æ—¶é—´åºåˆ—<b> time  dependent data</b>
- <b>æ— ç›‘ç£æˆ–å¼ºåŒ–å­¦ä¹ </b>æ—¶ï¼Œå¿…é¡»æ˜ç¡®â€œç½‘ç»œè¾“å‡ºåˆ°åº•ä»£è¡¨ä»€ä¹ˆâ€ what the result of the networks shall be.

æ¢å¥è¯è¯´ï¼š
<b>å¾ªç¯ç½‘ç»œä¸æ˜¯å¯¹å•ä¸ªæ ·æœ¬åšæ˜ å°„ï¼Œè€Œæ˜¯å¯¹æ•´ä¸ªæ—¶é—´åºåˆ—å»ºæ¨¡ã€‚</b>

1. Why not Backpropagation

> <b>ç»å…¸ BP ä¸èƒ½ç”¨äºå¾ªç¯ç½‘ç»œï¼Œå› ä¸ºåé¦ˆè¿æ¥ä¼šå¯¼è‡´æ— é™åå‘ä¼ æ’­ã€‚</b>

åŸå› ï¼š

- å¾ªç¯ç½‘ç»œå†…éƒ¨æœ‰ feedbackï¼ˆä¾‹å¦‚ y(tâˆ’1) â†’ context â†’ ä¸‹ä¸€æ­¥è¾“å…¥ï¼‰
- å¦‚æœç›´æ¥åå‘ä¼ æ’­ï¼Œè¯¯å·®ä¼šæ²¿ç€å¾ªç¯ç»“æ„æ— é™å›æº¯â†’ <b>æ¢¯åº¦æ°¸è¿œä¸ä¼šåœæ­¢ä¼ æ’­</b>

<b>feedback loop would cause an infinite back-propagation</b>

ä¸‰ç§å¸¸è§çš„å¾ªç¯ç½‘ç»œè®­ç»ƒæ–¹æ³•

1. BPTTï¼ˆBackpropagation Through Timeï¼‰ä¹Ÿå« <b>æ—¶é—´å±•å¼€ï¼ˆUnfolding in Timeï¼‰</b>
2. RTRLï¼ˆReal-Time Recurrent Learningï¼‰
    - RTRL æ˜¯ä¸€ç§ <b>åœ¨çº¿å­¦ä¹ ç®—æ³•</b>
    - æ¯æ¥ä¸€ä¸ªæ—¶é—´æ­¥å°±æ›´æ–°ä¸€æ¬¡æƒé‡
    - è®¡ç®—é‡éå¸¸å¤§ï¼ˆå¤æ‚åº¦ $O(n^4)$ï¼‰

3. Open Loop Learning / Teacher Forcing

#### 
#### 
