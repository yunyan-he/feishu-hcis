---
title: Basic problem
slug: GzC3wLxQeiAc7Ukf1WGcl4kzndb/UPgbwFzCAiYbQmksdS2cCTOBnrh
sidebar_position: 0
---


# Basic problem

> My name is He Yunyan. You can just call me Yann.
> I majored in Computer Science during my undergraduate studies. For my master's, I chose Human-Centered Intelligent Systems, which is closely aligned with my interests. I want to study Artificial Intelligence with a strong focus on Human-Centered applications. Thatâ€™s why I see Mixed Reality as an exciting direction â€” itâ€™s a way to bring AI into immersive, human-centered environments.
> ç†å¿µï¼ˆaiç‰ˆï¼‰ï¼š
> My goal is not only to explore AI in theory, but to make sure the systems we build can truly help people in real life. For example, while tools like GPT are widely used by office workers and writers, I believe there is still a huge gap in applying AI to production environments. Many industrial processes remain mechanical and repetitive, without real intelligence. I am very interested in exploring how AI can be designed to support workers and create practical impact in everyday contexts.
> ç†å¿µï¼ˆmrç‰ˆï¼‰
> My goal is not only to explore AI in theory, but to ensure that intelligent systems can truly help people in real life. Mixed Reality provides an exciting way to achieve this, by embedding AI into immersive, human-centered environments. While tools like GPT are widely used by office workers and writers, I believe there is still a huge gap in applying AI to production contexts. Many industrial processes remain mechanical and repetitive, without real intelligence. I am very interested in exploring how AI, combined with Mixed Reality, can be designed to support workers and create tangible impact in everyday environments.

> My most significant experience comes from my undergraduate work. I successfully completed a full-cycle project where I trained a <b>PyTorch model</b> and then <b>deployed it onto </b> <b>Android</b> 
> The primary goal of this project was not the final app itself, but rather to <b>master the entire AI application pipeline</b>: from data preparation and model training to the crucial steps of <b>on-device deployment and optimization</b> (what we call Edge AI). This process proved my ability to take a concept and <b>engineer a working, end-to-end solution</b>.
> To enhance this project, I recently integrated powerful external models, specifically utilizing the <b>Gemini family of models</b> via their API, creating a robust, hybrid cloud-and-edge system.
> åæ€ï¼š
> My project used Android for deployment, but through that process, I realized that the choice of platformâ€”whether it's a <b>native app</b>, a <b>web portal</b>, a <b>WeChat Mini Program</b> (like we use in China), or even models published via <b>emerging AI marketplaces</b> (like Baidu Smart Agent or the new model platforms) â€” is ultimately a strategic decision driven by the <b>user's context</b> and the <b>task requirements</b>.
> I only download apps for some very practical functions, or for those that are very rare and have no substitutes. Afterwards, people will choose to download apps. But I don't think animal recognition should be presented in the form of an app. However, when designing this, actually, implementing it with an Android app was a hard requirement at that time. But later I did some reflection on my own. So I think the significance of my work in this regard lies more in the exploration process, such as how to train a model, export it, and embed it into the app for invocation.
> In my work, I learned that purely <b>Edge AI</b> deployment often faces limitations, especially regarding model size and accuracy. That's why I advocate for a <b>hybrid approach</b>. We can run time-sensitive, core tasks locally (like motion tracking for MR) and delegate more complex, high-accuracy perception tasks to a powerful <b>cloud API</b>, such as Gemini. This balances <b>latency</b> and <b>performance</b>.

> å…³äºmrçš„æ€è€ƒï¼š
> To me, Mixed Reality is the perfect testbed for <b>real-time AI</b>. To blend the virtual and real worlds seamlessly, we need extremely robust and low-latency <b>computer vision</b> (for tracking and environment understanding) and <b>natural interaction systems</b> (like gesture recognition). My experience in optimizing models for speed directly translates to the demands of MR.
> OK, so actually these days, a lot of applications have given me a lot of insights. I always use a lot of artificial intelligence apps, and I find that after years of development, these apps have become more and more compatible in input methods. You can input text, which is the basic one, right? But you can also input some voice information, and now you can input video information, like having a video call with them. So when they receive this information, they can give you very quick feedback. Sometimes this information overlaps on the screen. It gives you a sense that on the screen, it's a mix of the real world and the virtual world."

> æœªæ¥è®¡åˆ’ï¼š
> Regarding my future plans, first of all, I haven't decided which field I want to devote myself to in the future. Because I believe people's energy is limited, and I have to choose an area that I like and in which I have some ability. So I'm very cautious about long-term plans. In the short term, my goal is to broaden my horizons, explore new fields, and learn about different possibilities. Because I believe only after a preliminary understanding and experience can you know if you want it. For example, before, you might think that you will engage in scientific research in the future, pursue a Ph.D., and settle in this field. But I don't think getting a Ph.D. and conducting in-depth research is something that requires a lot of knowledge, experience, endurance, and stress resistance. And you also need to learn how to deal with your anxiety. At the same time, it requires you to have a very high level of curiosity about this field. Because if you can't raise a good question, you can't conduct good research. So I think everything should be about suitability, but you must try to know if it's suitable. Just like only the foot knows if the shoe fits. That is to say, you must take the first step and have a preliminary attempt to know if you are suitable.

I havenâ€™t fixed my long-term career plan yet, because I believe suitability must be tested through practice. In the short term, I want to broaden my horizons and gain real experience. My philosophy is clear: I want to build practical technologies that people can use in daily life. I hope to make technology accessible and turn code into tangible products that people can interact with.

### ä¸ªäººä¿¡æ¯

> My Career Development Views (Organized Version) 
> Regarding my career development plan, direction and time scale are two key factorsâ€”choosing the right field and deciding how long to stick to it.
> First, on career directions, I have three possible paths in mind. The first is working in the industry as a developer, such as a full-stack, front-end, back-end, data developer, or AI engineer. The second is pursuing academic research, meaning continuing to a PhD after my masterâ€™s (either industry-collaborated PhD, institute-based PhD, or university PhD). The third is a mixed path: working first then doing a PhD later, or going into industry after getting a PhD. Ultimately, my career will lie in either industry, academia, or a cross of both. 
> Second, in terms of time scale, my masterâ€™s program is a crucial period. During this time, I need to choose courses based on my target direction and actively seize opportunitiesâ€”internships, teaching assistant roles, etc. But since I canâ€™t confirm my direction quickly in 2-3 years, my short-term plan (next 6-12 months) is to find my true direction through practice. I believe assumptions about a field (like judging a course only by its title) are unreliable; real experience is key. So Iâ€™ll take real tasks, get to know how academia and industry work in practice to make informed choices.
> For long-term plans, I donâ€™t have a fixed one yetâ€”Iâ€™m still exploring. But my core value/philosophy has always been: I want to create practical things that people can use in daily life. I hope to make technology accessible and turn code into tangible products that people can interact with.

---

## å®Œæ•´ç¤ºèŒƒå›ç­” (Complete Sample Answers)

### ğŸ” æŠ€æœ¯ä¸é¡¹ç›®ç»éªŒ (Technical & Project Competence)

<b>(Q1, Q2, Q3 å·²åœ¨å‰é¢å›ç­”ï¼Œè¿™é‡Œä¸å†é‡å¤)</b>

### ğŸ§  ç ”ç©¶å…´è¶£ä¸çŸ¥è¯†å¹¿åº¦ (Research Interest & Knowledge Breadth)

#### Q1: How do you see your <b>Edge AI experience connecting</b> to our Mixed Reality research?

> <b>A1:</b> I see a direct and vital connection. Mixed Reality applicationsâ€”especially those focusing on natural Human-Computer Interactionâ€”are fundamentally reliant on <b>low-latency, on-device perception</b>. My experience is precisely in achieving this: optimizing models for speed, managing resource constraints, and ensuring functional, low-latency deployment. In MR, we need instantaneous processing for tasks like <b>hand tracking, gaze estimation, and environment understanding</b>. My skills can be immediately applied to optimizing the <b>perception backbone</b> that drives your MR systems.

#### Q2: What are your thoughts on recent developments in <b>large vision models (like Gemini)</b>?

> <b>A2:</b> I find the developments in large multi-modal models fascinating, and I view them as a <b>complementary technology</b> to Edge AI. They offer unparalleled high-level reasoning and comprehensive semantic understanding via the cloud. This trend strongly validates the <b>hybrid AI approach</b> I explored in my project. In MR, we can utilize fast, efficient Edge models for core, high-frequency tasks (like tracking) and delegate complex, occasional semantic queries (e.g., identifying obscure objects or generating content) to these powerful cloud APIs. It allows us to balance <b>real-time performance</b> with <b>cognitive intelligence</b>.

#### Q3: If you were to start a new project in MR today, what specific problem would you try to solve?

> <b>A3:</b> Given my background in Edge AI and my focus on Human-Centered Systems, I would target the problem of <b>robust, personalized gesture recognition</b>. Many current gesture systems rely on rigid, pre-defined sets. I would propose a project focusing on creating a lightweight, on-device model that can <b>quickly adapt and learn new, unique user gestures</b> (perhaps based on few-shot learning) to control digital elements in an MR environment. The goal would be to move beyond standardized commands to genuinely <b>personalized and intuitive interaction</b>â€”a key principle of HCIS.

#### Q4: Why are you passionate about <b>real-time AI interaction</b>?

> <b>A4:</b> My passion stems from the idea of <b>seamless user experience</b>. For any intelligent system to truly augment human lifeâ€”whether it's an app or an AR headsetâ€”it cannot impose cognitive load or noticeable delay. Real-time AI interaction is the prerequisite for <b>trust and intuitiveness</b>. It moves the technology out of the way, allowing the user to focus on the task itself. Mixed Reality, by demanding near-zero latency, represents the ultimate challenge and reward for anyone passionate about building truly intuitive human-computer interfaces.

### ğŸ§© å²—ä½åŒ¹é…åº¦ä¸æ–‡åŒ–å¥‘åˆ (Fit & Motivation)

#### Q6: Why do you want to work at <b>Fraunhofer</b>? / Why this specific research group?

> <b>A6:</b> I'm drawn to Fraunhofer's position at the intersection of <b>fundamental research and industrial application</b>. I'm passionate about <b>applied research</b> that has a tangible impact. Your specific focus on <b>Mixed Reality</b> is incredibly compelling because it's the cutting edge of human-computer interaction, and it aligns perfectly with my goal to develop robust, <b>real-time interactive AI systems</b>. I want to be part of a team that is defining the future of technology.

#### Q7: What specific skills or tools (e.g., Unity/ARKit) are you eager to learn here?

> <b>A7:</b> I am proficient in the AI/ML pipeline, but I am very eager to master the <b>specific implementation frameworks</b> used in your group. I want to rapidly get up to speed with platforms like <b>Unity or Unreal Engine</b>, and dive into their SDKs (like ARKit or ARCore), so I can effectively translate my optimized Edge AI models directly into functional <b>Mixed Reality experiences</b> within your project environment.

#### Q8: What is your expected time commitment per week for a HiWi position?

> <b>A8:</b> I am very flexible and committed to contributing meaningfully to the team. I am looking for a typical HiWi commitment, perhaps <b>10 to 15 hours per week</b>, but I am willing to discuss and adjust based on the project's requirements and my academic schedule to ensure I can consistently meet deadlines and deliver results.

#### Q9: How do you prefer to collaborate on a research project?

> <b>A9:</b> I value clear communication. I prefer to have <b>well-defined, weekly goals</b> and regular check-ins to ensure alignment. I'm comfortable working independently on technical tasks, but I always seek feedback and discussion when designing the core architecture or encountering a major roadblock. I see collaboration as essential for testing assumptions and ensuring the project stays on track.

### ğŸš€ å‡è®¾æƒ…æ™¯ä¸è¡Œä¸ºé—®é¢˜ (Hypothetical & Behavioral)

#### Q10: Imagine you have a new AR project, but the budget is very limited. How would you prioritize the technical scope?

> <b>A10:</b> With limited resources, the priority must be the <b>Minimum Viable Product (MVP)</b>. I would first focus on the <b>core functionality</b>â€”for MR, that means robust, low-latency tracking and interaction (e.g., the hand gesture recognition). I would use open-source libraries or foundational tools (like MediaPipe) where possible, and <b>defer</b> complex, resource-intensive features (like high-fidelity environment reconstruction or advanced rendering) until more funding is secured. <b>Functionality over polish</b> would be the guiding principle.

#### Q11: If a research result goes against your initial hypothesis, what would be your next step?

> <b>A11:</b> That's common in research. My first step would be to ensure the result is <b>reproducible and statistically valid</b>. I would then move to a systematic <b>root cause analysis</b>: checking the data, reviewing the experimental setup, and critically evaluating my initial assumptions. Rather than seeing it as a failure, I view it as an <b>opportunity to learn something new</b> about the problem domain that I hadn't considered, which often leads to the most interesting research findings.

#### Q12: Have you ever had a disagreement with a project supervisor/teammate? How did you handle it?

> <b>A12:</b> Yes, during my graduation project, my teammate and I disagreed on whether to prioritize model <em>accuracy</em> or <em>speed</em> for the mobile app. I handled it by moving the discussion to a <b>data-driven basis</b>. We ran A/B tests on two different deployed models and presented the results to our supervisor, linking the trade-offs directly to the <b>user experience</b> and the <b>project's core goal</b> (which was low latency). We ultimately compromised by using the faster model for the main feature and reserving the high-accuracy model for a cloud-based backup. We prioritized the project goal over personal preference.

---

### Future plan

#### Q1: How do you see this HiWi position fitting into your future career path?

> <b>A1 (å¼ºè°ƒå­¦ä¹ å’Œå®šä½):</b> This HiWi position is <b>critical</b> for me right now. As I've just started my Master's, I need practical exposure to help me navigate the HCIS curriculum and narrow down my specialization. Working with your team on Mixed Reality will allow me to immediately apply the <b>Edge AI skills</b> I already possess, while learning the specific tools and research methodology (like Unity/Unreal) needed for my <b>future Master's thesis</b> in the domain of Human-Computer Interaction. This is the fastest way for me to get started in research.

#### Q2: Where do you see yourself in five years?

> <b>A2:</b> In five years, I aim to have completed my Master's (hopefully with a thesis supervised by this group, if possible!) and be working as an applied researcher or advanced engineer. My ultimate goal is to work on <b>intelligent, user-centric systems</b>â€”the kind that seamlessly augment human capabilities, which is exactly what your Mixed Reality research is striving to achieve.

#### Q3: (æ½œåœ¨é—®é¢˜) How will you balance the HiWi commitment with your new Master's studies?

> <b>A3:</b> I am fully aware that the first semester at RWTH is challenging. I am committed to dedicating a manageable amount of time, perhaps <b>10-12 hours per week</b>, to the HiWi role. I am highly organized, and I see this work as a <b>complementary learning experience</b>, not a distraction. By working on the practical applications here, I believe it will even help me better understand the theoretical concepts taught in my HCIS courses.

## Fastai

### Why fastai

User-friendly api : First, FastAI is extremely user - friendly. Even as a beginner with limited machine - learning knowledge, I was able to start building my model in no time. It doesn't overwhelm you with complex concepts right off the bat.

Convenient and rapid prototyping, quickly create and adjust a basic version of my model, easily to test different architectures
- Tries resnet 18 /34/50/101   SDG/Adam 

(explain convenient prototyping)Build-in training pipeline, a lot of pre-set steps to finish the training process.

- Build-in Data augmentation  it offers a set of ready - to - use methods to expand your training data.
    - it provides a plethora of high - encapsulated APIs. The built - in data augmentation API, for instance, made it so easy to expand my training data. I didn't need to write complex code to perform data transformations; I could just use the pre - built functions.

- Built-in transfer learning in FastAI makes it very easy to use pre-trained models.
    - can take a model that has already been trained on a large-scale dataset, like ImageNet for image-related tasks. Then, you can fine-tune this pre-trained model on your specific dataset. This way, you don't have to start from scratch and can achieve good results with less data and training time.
    - Finally, transfer learning is a real game - changer. FastAI's support for it allowed me to use pre-trained models. This way, I could achieve good accuracy for my animal recognition project with a relatively small dataset and in a short period. It significantly sped up my development process."

> I used FastAI for an animal recognition project, where the goal was to train a model to identify specific animal species from images. I chose FastAI because it is very user-friendly and allows rapid prototyping. Its high-level APIs simplify many common tasks, and it provides built-in pipelines for data augmentation, transfer learning, and model training, which made the process smooth and efficient.

### Data process

How did you preprocess your data before using FastAI? Did you encounter any issues with data quality and how did you address them? What data augmentation techniques did you use and why did you choose them? And, how did you split your data into training, validation, and test sets?

Well, before using FastAI, I first conducted a series of pre - processing steps. I scanned all the photos to digitize them. Then, I manually removed some photos that were clearly not real animals. (this is because my dataset is not very large when the dataset is very large, we will use other method, like training a model to do this task, use statistic method, or use crowd-scourcing platform )This was necessary because such images would only mislead the model during training.
<em>When it came to data quality issues, one major problem was the presence of blurry or extremely low - resolution pictures. To solve this, I used image processing libraries to check the resolution of each photo. If the resolution was too low, I discarded the photo. For blurry images, although there were not many, I also removed them as they could degrade the model's performance.</em>
As for data augmentation techniques, I employed resizing, rotating, and flipping. Resizing was crucial because in the real world, animals can appear in different sizes in photos. By resizing the images to a standard size, the model can learn to recognize the animals regardless of their initial scale. Rotating the pictures helps the model understand an animal's appearance from various angles. Just like in real - life scenarios, we might see an animal from the side, front, or back. Flipping the images is also useful as it can double the amount of training data in a simple way, and it mimics the symmetry that often exists in animal bodies.(these augmentation techniques are relatively easy to implement in FastAI. The library provides built - in functions to perform these operations with just a few lines of code, which further contributed to the efficiency of your data pre - processing.)
For splitting the data, I used FastAI's built - in functionality. I set the training set to 70% of the total data. This large portion ensures that the model has enough samples to learn the patterns in the data. The validation set takes up 15%, and it's used to fine - tune the hyperparameters of the model. This allows me to evaluate how well the model is performing during the training process and make necessary adjustments. The remaining 15% is for the test set. I use this set to get an unbiased assessment of the final model's performance, as it hasn't been used during training or validation.

> I used two datasets. The first one was downloaded from Kaggle. It contains 90 species of animals. The second one is the Oxford Pet dataset, which has 37 different kinds of dogs or cats. The quality of the first dataset wasn't as high as that of the Oxford Pet dataset. So, I manually cleaned the data. I removed images such as paintings and black - and - white photos. After cleaning the data, I applied FastAIâ€™s built-in data augmentation techniques, including zooming, rotation, and adjusting lighting. Data augmentation effectively expands the dataset, allowing the model to learn more robust features and generalize better to unseen images.
> Then i use <b>DataBlock</b><b>ï¼ˆ</b>high-level API in FastAIï¼‰ to define a data pipeline.
> Parameter meanings:
> - `blocks=(ImageBlock, CategoryBlock)`: input = images, output = labels
> - `get_items=get_image_files`: get all images in the folder
> - `splitter=RandomSplitter(valid_pct=0.2, seed=42)`: randomly split into train/validation, 20% validation
> - `get_y=parent_label`: generate labels from parent folder names
> - `item_tfms=[Resize(224, method='squish')]`: resize each image to 224Ã—224
> - `batch_tfms=custom_transforms`: apply previously defined augmentations to batch
> `dataloaders(path, bs=32)`: create DataLoader with batch size 32
> `show_batch(max_n=15)`: visualize first 15 samples
> Next, I created a `DataBlock` to define the data pipeline. è¾“å…¥æ˜¯å›¾ç‰‡ï¼ˆ`ImageBlock`ï¼‰ï¼Œè¾“å‡ºæ˜¯ç±»åˆ«æ ‡ç­¾ï¼ˆ`CategoryBlock`ï¼‰ï¼Œé€šè¿‡ `get_items=get_image_files` è·å–æ‰€æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨ `parent_label` ä»çˆ¶æ–‡ä»¶å¤¹ç”Ÿæˆæ ‡ç­¾ã€‚
 æˆ‘ç”¨ `RandomSplitter(valid_pct=0.2)` éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå æ¯” 80/20ã€‚æ¯ä¸ªæ ·æœ¬é€šè¿‡ `item_tfms=[Resize(224)]` ç»Ÿä¸€è°ƒæ•´å¤§å°ï¼Œbatch ä½¿ç”¨ `batch_tfms=custom_transforms` åº”ç”¨å¢å¼ºã€‚
 Finally, `dataloaders(path, bs=32)` ç”Ÿæˆ DataLoaderï¼Œæ¯ batch 32 å¼ å›¾ï¼Œ`show_batch(max_n=15)` å¯ä»¥å¿«é€Ÿå¯è§†åŒ–æ ·æœ¬ã€‚â€

### Model training and optimization

How did you choose the learning rate for your model training? Did you face any issues with the model overfitting or underfitting, and if so, how did you deal with it? What optimizer did you use and why? How long did it take for the model to converge, and what steps did you take to speed up the training process?



How did you choose the learning rate for your model training?

You could say that you started with a default learning rate provided by FastAI, which is often a good starting point. Then, you used techniques like learning rate scheduling. For example, the OneCycle LR method. This method gradually increases the learning rate during the first part of training and then decreases it during the latter part. By doing this, you can find the optimal learning rate that helps the model converge faster and achieve better performance.

`/lr.find()` is a powerful tool in FastAI that helps you automatically find an optimal learning rate range. It does this by gradually increasing the learning rate during a short training run and monitoring the loss. It then returns a range of learning rates within which the model is likely to converge quickly and perform well. This method saves a lot of time compared to manually tuning the learning rate through trial and error.



Did you face any issues with the model overfitting or underfitting, and if so, how did you deal with it?

If you faced overfitting: You can mention that you used techniques such as adding dropout layers. Dropout randomly sets a fraction of input units to zero during training, which forces the model to learn more robust features. Another approach could be using L2 regularization. It adds a penalty term to the loss function that discourages the weights from becoming too large, preventing the model from over - fitting to the training data.
If it was underfitting: You might say that you increased the complexity of the model. For example, if you were using a simple neural network, you could add more layers or increase the number of neurons in each layer. You could also try using more data through data augmentation or getting additional real - world data.

What optimizer did you use and why?

If you used Adam optimizer, you can explain that Adam combines the advantages of Adagrad and Adadelta. It adapts the learning rate for each parameter, which is very effective for many deep - learning tasks. It converges faster compared to some traditional optimizers like Stochastic Gradient Descent (SGD). It also requires less tuning of the learning rate, making it a convenient choice for beginners and experienced practitioners alike.

How long did it take for the model to converge, and what steps did you take to speed up the training process?



First, I used `lr.find()` in FastAI to find the optimal learning rate, which helps the model converge faster. Then, I employed early stopping. I set it up so that once the model converged for about 5 epochs, the training would end.

In addition, I utilized a T4 GPU on Colab. FastAI is well - optimized for GPU acceleration, and using this powerful hardware significantly reduced the training time. These combined efforts not only helped in getting the model to converge in a reasonable time but also made the overall training process more efficient

> Batch size adjust
> After preparing the dataset and DataLoaders, I created a vision learner using FastAIâ€™s `vision_learner` with a ResNet101 backbone. I chose it because it strikes a good balance between accuracy and model complexity, making it highly suitable for my animal - recognition task.æˆ‘é€‰æ‹© ResNet101 æ˜¯å› ä¸ºå®ƒåœ¨å‡†ç¡®ç‡å’Œæ¨¡å‹å¤æ‚åº¦ä¹‹é—´æœ‰è‰¯å¥½çš„å¹³è¡¡ï¼Œéå¸¸é€‚åˆæˆ‘çš„åŠ¨ç‰©åˆ†ç±»ä»»åŠ¡ã€‚
 I set accuracy as the main metric and added `CSVLogger` to record the training process for later analysis.â€
> â€œBefore training, I used FastAIâ€™s `lr_find()` to search for the optimal learning rate for stable and fast convergence.(æˆ‘é€‰æ‹© loss æ›²çº¿å¿«é€Ÿä¸‹é™çš„ç‚¹ä½œä¸ºæœ€ä½³å­¦ä¹ ç‡I choose the point where the loss curve drops rapidly as the optimal learning rate) 
 é¢è¯•å®˜å¯èƒ½ä¼šé—®ï¼Œå¦‚æœå­¦ä¹ ç‡å¤ªå¤§æˆ–å¤ªå°ä¼šå‘ç”Ÿä»€ä¹ˆâ€”â€”å¤ªå¤§ä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œå¤ªå°åˆ™æ”¶æ•›ç¼“æ…¢ã€‚â€If the learning rate is excessively large, it may lead to gradient explosion. Conversely, if the learning rate is too small, the convergence speed might be extremely slow.
> â€œThen I used `fine_tune()` for transfer learning, training the pretrained ResNet101 on my dataset for 15 epochs with  2 callbacks:
> - `EarlyStoppingCallback` stops training if validation loss doesnâ€™t improve for 4 epochs, preventing overfitting.`EarlyStopping Callback` ç›‘æ§éªŒè¯é›† lossï¼Œå¦‚æœè¿ç»­ 4 ä¸ª epoch æ²¡æœ‰ä¸‹é™å°±æå‰åœæ­¢è®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
> - `SaveModelCallback` saves the model with the lowest validation loss to ensure the best model is reserved.â€ä¿å­˜éªŒè¯é›† loss æœ€å°çš„æ¨¡å‹ï¼Œä¿è¯æœ€ç»ˆå¾—åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œè€Œä¸æ˜¯æœ€åä¸€ä¸ª epoch çš„æ¨¡å‹ã€‚â€

### Onnx

That's a great choice! You can tell them that first, you exported your FastAI model to the ONNX format. FastAI has built - in functions that make this export process quite straightforward. Once you had the ONNX model, it's very convenient for deployment on Android devices. ONNX is an open standard for machine - learning models, which means it's highly compatible with the Android ecosystem.
On the Android side, you can use the ONNX Runtime library. You first integrate this library into your Android project. Then, you can load the ONNX model into your app at runtime. This allows your Android app to perform inference using the pre - trained model. It's a great way to achieve high - performance machine - learning capabilities on mobile devices because ONNX Runtime optimizes the model execution for different hardware, ensuring smooth and efficient operation

> After training the model in FastAI, I exported it to PyTorch and then converted it to ONNX format for deployment. ONNX allows cross-platform compatibility, which is ideal for mobile or embedded systems.
> During conversion, I ensured that the input shapes were fixed and compatible with ONNX, and I used an appropriate opset version to support all necessary operations. After exporting, I applied <b>ONNX graph simplification</b> to remove redundant nodes and streamline computation.
> To optimize for mobile inference, I applied <b>dynamic int8 quantization</b>, which reduced the model size significantly (for example, from 20MB to around 5MB) and improved CPU inference speed without substantial loss of accuracy. I also considered other options like FP16 for GPU inference and pruning less important channels, but dynamic int8 offered the best trade-off for mobile devices.
> Finally, the optimized ONNX model was integrated into an Android app using <b>ONNX Runtime</b>, enabling real-time image recognition. This workflow ensures that a model trained in Python can run efficiently on mobile devices, which is directly applicable to MR/AR prototypes that require fast on-device perception.

### challange

- é‡åˆ°è¿‡ä»€ä¹ˆé—®é¢˜ï¼Ÿæ€ä¹ˆè§£å†³ï¼Ÿ
- ä¾‹å¦‚ï¼šè®­ç»ƒæ…¢ã€å†…å­˜ä¸å¤Ÿã€éƒ¨ç½²åˆ° mobile çš„é—®é¢˜

## Android 

### âœ… æ¨¡å‹æ€ä¹ˆä»è®­ç»ƒç«¯ â†’ éƒ¨ç½²åˆ° Android

#### ä½ è®­ç»ƒå¥½çš„æ¨¡å‹å¦‚ä½•éƒ¨ç½²åˆ°ç§»åŠ¨ç«¯ï¼Ÿ

#### ç”¨çš„æ˜¯ ONNX / TensorFlow Lite / PyTorch Mobileï¼Ÿ

#### æ¨¡å‹å¤§å°å¤šå°‘ï¼Ÿæ¨ç†é€Ÿåº¦å¤šå°‘ï¼Ÿ

### âœ… æ¨ç†æ€ä¹ˆè·‘ï¼ˆCPU / GPU / NNAPIï¼‰

#### ç”¨ä»€ä¹ˆæ–¹å¼åŠ è½½æ¨¡å‹ï¼Ÿ

I use ONNX Runtime on Android by initializing an `OrtEnvironment` and creating `OrtSession` objects. The model is loaded from assets into internal storage and then executed using `session.run()` with tensors created manually. This allows fully offline inference on mobile devices.

### âœ… æ¨ç†å»¶è¿Ÿã€å†…å­˜å ç”¨ã€åŠŸè€—

#### æ˜¯å¦åšè¿‡å¤šçº¿ç¨‹æ¨ç†

### âœ… æ˜¯å¦é˜»å¡ UI çº¿ç¨‹

Crucially, I decoupled the inference logic from the main UI thread. I used <b>Kotlin Coroutines</b> with the `Dispatchers.IO` context to perform the heavy liftingâ€”specifically the model loading and the `recognizeAnimal` functionâ€”asynchronously. This ensures that the inference process never blocks the UI, which is a critical requirement for maintaining  a smooth UI flowã€‚ <i>Translation:</i> è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘å°†æ¨ç†é€»è¾‘ä¸ä¸» UI çº¿ç¨‹è§£è€¦ã€‚æˆ‘ä½¿ç”¨äº† <b>Kotlin åç¨‹</b> å’Œ `Dispatchers.IO` ä¸Šä¸‹æ–‡æ¥å¼‚æ­¥æ‰§è¡Œç¹é‡çš„ä»»åŠ¡â€”â€”å…·ä½“æ¥è¯´æ˜¯æ¨¡å‹åŠ è½½å’Œ `recognizeAnimal` å‡½æ•°ã€‚è¿™ç¡®ä¿äº†æ¨ç†è¿‡ç¨‹ç»ä¸ä¼šé˜»å¡ UIï¼Œè¿™å¯¹äºç»´æŒåº”ç”¨ç•Œé¢æµç•…æ˜¯ä¸€ä¸ªå…³é”®è¦æ±‚ã€‚

### âœ… æ˜¯å¦æ”¯æŒæ‰¹é‡ or å®æ—¶æ¨ç†

### ONNX Runtime åŸç†

### çº¿ç¨‹ä¸æ€§èƒ½ä¼˜åŒ–

### Latency & Performance Monitoring

Yes, performance monitoring is embedded in my inference loop. I used `System.nanoTime()` to capture precise timestamps before and after the `model.run()` call to calculate the exact inference duration in milliseconds. <i>Translation:</i> æ˜¯çš„ï¼Œæ€§èƒ½ç›‘æ§åµŒå…¥åœ¨æˆ‘çš„æ¨ç†å¾ªç¯ä¸­ã€‚æˆ‘ä½¿ç”¨ `System.nanoTime()` åœ¨ `model.run()` è°ƒç”¨å‰åæ•æ‰ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œä»¥è®¡ç®—æ¯«ç§’çº§çš„ç¡®åˆ‡æ¨ç†æ—¶é•¿ã€‚

### ç«¯ä¾§ AI é™åˆ¶

### Data process

I implemented a strict preprocessing pipeline to match the model's training requirements. The raw `Bitmap` is first scaled to the target resolution of 224x224. Then, I manually convert the pixel data into a `FloatArray`, applying standard normalization with a specific mean and standard deviation (Mean: 0.485, 0.456, 0.406; Std: 0.229, 0.224, 0.225). <i>Translation:</i> æˆ‘å®ç°äº†ä¸€ä¸ªä¸¥æ ¼çš„é¢„å¤„ç†ç®¡çº¿æ¥åŒ¹é…æ¨¡å‹çš„è®­ç»ƒè¦æ±‚ã€‚åŸå§‹çš„ `Bitmap` é¦–å…ˆè¢«ç¼©æ”¾åˆ° 224x224 çš„ç›®æ ‡åˆ†è¾¨ç‡ã€‚ç„¶åï¼Œæˆ‘æ‰‹åŠ¨å°†åƒç´ æ•°æ®è½¬æ¢ä¸º `FloatArray`ï¼Œå¹¶åº”ç”¨ç‰¹å®šçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆMean: 0.485... Std: 0.229...ï¼‰è¿›è¡Œæ ‡å‡†åŒ–

Finally, this float array is wrapped into a `FloatBuffer` and converted into an `OnnxTensor` with a shape of `[1, 3, 224, 224]`. This tensor serves as the input for the `ortSession.run()` method. This manual pipeline gives me full control over memory allocation and allows for future optimization using native C++ buffers if needed. <i>Translation:</i> æœ€åï¼Œè¿™ä¸ªæµ®ç‚¹æ•°ç»„è¢«å°è£…è¿› `FloatBuffer` å¹¶è½¬æ¢ä¸ºå½¢çŠ¶ä¸º `[1, 3, 224, 224]` çš„ `OnnxTensor`ã€‚è¿™ä¸ª Tensor è¢«ç”¨ä½œ `ortSession.run()` æ–¹æ³•çš„è¾“å…¥ã€‚è¿™ç§æ‰‹åŠ¨ç®¡çº¿è®©æˆ‘èƒ½å®Œå…¨æ§åˆ¶å†…å­˜åˆ†é…ï¼Œå¹¶ä¸”åœ¨æœªæ¥éœ€è¦æ—¶å¯ä»¥ä½¿ç”¨åŸç”Ÿ C++ ç¼“å†²åŒºè¿›è¡Œä¼˜åŒ–ã€‚

### â˜ï¸ Part 4: The Hybrid Strategy (Edge + Cloud) (æ··åˆæ¶æ„ç­–ç•¥)

<b>Professor's Likely Question:</b><em>"You mentioned using Gemini API as well. How does your system decide when to use the local model versus the cloud API?"(ä½ æåˆ°äº†ä½¿ç”¨ Gemini APIã€‚ä½ çš„ç³»ç»Ÿå¦‚ä½•å†³å®šä½•æ—¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä½•æ—¶ä½¿ç”¨äº‘ç«¯ APIï¼Ÿ)</em>

<b>Your Answer:</b>

"I designed a fallback mechanism to balance latency and intelligence. The system prioritizes the local ONNX model for immediate, low-latency feedback. However, if the local recognition fails (returns an 'Unrecognized' state) or if the user requests deeper contextual details, the system triggers the cloud path. <i>Translation:</i> æˆ‘è®¾è®¡äº†ä¸€ä¸ªå›é€€æœºåˆ¶æ¥å¹³è¡¡å»¶è¿Ÿå’Œæ™ºèƒ½ã€‚ç³»ç»Ÿä¼˜å…ˆä½¿ç”¨æœ¬åœ° ONNX æ¨¡å‹ä»¥è·å¾—å³æ—¶ã€ä½å»¶è¿Ÿçš„åé¦ˆã€‚ç„¶è€Œï¼Œå¦‚æœæœ¬åœ°è¯†åˆ«å¤±è´¥ï¼ˆè¿”å›â€œæœªè¯†åˆ«â€çŠ¶æ€ï¼‰æˆ–è€…ç”¨æˆ·è¯·æ±‚æ›´æ·±å±‚çš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œç³»ç»Ÿä¼šè§¦å‘äº‘ç«¯è·¯å¾„ã€‚"

"In that case, I encode the image to Base64 and send it to the Gemini API asynchronously. This hybrid architecture allows the application to be 'Real-Time' for basic interaction, but 'Intelligent' for complex queries, which fits perfectly with the requirements of Mixed Reality assistants. <i>Translation:</i> åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘å°†å›¾åƒç¼–ç ä¸º Base64 å¹¶å¼‚æ­¥å‘é€ç»™ Gemini APIã€‚è¿™ç§æ··åˆæ¶æ„ä½¿å¾—åº”ç”¨åœ¨åŸºç¡€äº¤äº’ä¸Šæ˜¯â€œå®æ—¶â€çš„ï¼Œä½†åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶æ˜¯â€œæ™ºèƒ½â€çš„ï¼Œè¿™éå¸¸ç¬¦åˆæ··åˆç°å®åŠ©æ‰‹çš„éœ€æ±‚ã€‚"

Iâ€™d like to briefly present one of my main projects

The main motivation of this project was to explore how modern mobile systems can combine <b>on-device intelligence</b> and <b>cloud-based multimodal AI</b> in a practical and efficient way.

I was especially interested in edge AI in real-world mobile constraints.

From a system perspective, I designed the application as a <b>hybrid AI architecture</b>.
 There are two parallel inference paths:

The first path is fully on-device using ONNX Runtime, which provides fast, offline and privacy-preserving inference.

The second path uses a cloud-based AI model, Google Gemini, which provides more powerful multimodal understanding when network connectivity is available.

For the on-device pipeline, I implemented a <b>two-stage inference strategy</b>.

First, a lightweight general classifier identifies the animal category among 90 species.
 If the result is a cat or a dog, a second specialized model is dynamically activated to perform fine-grained breed recognition.

This staged design significantly reduces unnecessary computation and is suitable for mobile and wearable devices with limited energy budgets.

On the Android side, I implemented a preprocessing pipeline that strictly mirrors the transformations used during training.

All heavy computation and network communication are handled using Kotlin Coroutines, so the UI remains fully responsive even during inference.

In parallel, I integrated the Google Gemini multimodal API.

The system sends a base64-encoded image together with a structured prompt and parses the semantic output.
 This allows the user to compare the edge AI result with a large cloud model in real time.

In summary, this project helped me explore how to design efficient, robust, and scalable AI systems for real-world mobile and human-computer interaction scenarios.

One of the main challenges I encountered was designing a consistent pipeline between the training environment and the mobile inference environment.

In particular, I needed to make sure that the image preprocessing steps â€” such as resizing, normalization and channel ordering â€” were exactly aligned between FastAI during training and the Android ONNX Runtime execution.

If I were to redesign this project, I would focus more on systematic performance optimization and quantitative evaluation.

First, I would introduce structured benchmarking to measure latency, memory usage, and energy consumption under different model sizes and hardware backends, instead of relying mainly on qualitative observation.

Second, I would further optimize the on-device models by applying post-training quantization, such as INT8 quantization, and benchmark different runtimes such as NNAPI or GPU delegates to improve efficiency.

